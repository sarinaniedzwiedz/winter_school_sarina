{
  "hash": "3bae37772868c05e683f71ef894d1f12",
  "result": {
    "markdown": "---\ntitle: \"Bonus\"\neditor: source\n---\n\n::: {.cell}\n\n:::\n\n\n# Animations\n\nIn this supplemental tutorial we are going to look at how to create animations in R. To do this will require the installation of software outside of R. This software is `ImageMagick` and may be downloaded here: <https://www.imagemagick.org/script/download.php>. Once this software has been installed on your computer it will be necessary to install the `animation` library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The libraries required for this tut\nlibrary(tidyverse)\nlibrary(grid)\nlibrary(gridExtra)\n# install.packages(\"animation\")\nlibrary(animation)\n```\n:::\n\n\n## Functions for creating ant walks\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate speed based on u and v vectors\nant.speed <- function(df){\n  df$x2 <- c(NA,df$x[2:nrow(df)] - df$x[1:(nrow(df)-1)])\n  df$y2 <- c(NA,df$y[2:nrow(df)] - df$y[1:(nrow(df)-1)])\n  speed_abs <- round(sqrt(df$x2^2 + df$y2^2),2)\n  speed_abs[is.na(speed_abs)] <- 0\n  return(speed_abs)\n}\n\n# Create a dataframe with desired number of ants and steps\nant.walk <- function(i,n){\n  # Create the random walks\n  walk_x <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  for(i in 2:i){\n  x <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  walk_x <- c(walk_x, x)\n  }\n  walk_y <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  for(i in 2:i){\n  y <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  walk_y <- c(walk_y, y)\n  }\n  # Create the walking dataframe\n  walker <- data.frame(x = walk_x, y = walk_y, \n                       ant = as.factor(rep(1:i, each = n)), \n                       step =  rep(seq(1,n), i))\n  walker$speed <- ant.speed(walker)\n  walker$speed[walker$step == 1] <- 0\n  return(walker)\n}\n```\n:::\n\n\n## Generate the ants\n\n\n::: {.cell}\n\n```{.r .cell-code}\nants <- ant.walk(5, 100)\n```\n:::\n\n\n## The function to animate the walk plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwalk.plot <- function(i){\n  # Map figure\n  walk_map <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = x, y = y)) +\n    geom_path(aes( group = ant), colour = \"gray60\") +\n    geom_point(data = ants[ants$step == i,], aes(colour = ant))\n  # Speed histogram\n  walk_hist <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = speed)) +\n    geom_histogram() +\n    labs(x = \"speed\")\n  # Speed line graph\n  walk_line <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = step, y = speed)) +\n    geom_line(aes(colour = ant))\n  # Wack it together\n  grid.arrange(walk_map, walk_hist, walk_line, layout_matrix = cbind(c(1,1), c(1,1), c(2,3)))\n}\n\n\n## Create animation of ts plots\nanimate.walk.plot <- function() {\n  lapply(seq(1,100), function(i) {\n    walk.plot(i)\n  })\n}\n```\n:::\n\n\n## Render the GIF\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# By default 'saveGIF()' outputs to the same folder \n# the script where the code is being run from is located\n# I have included commented out lines of code here that \n# may be changed to set thedestination for saving the output\n# setwd(\"~/Intro_R_Workshop/BONUS/\")\nsystem.time(saveGIF(animate.walk.plot(), interval = 0.2, \n                    ani.width = 800, movie.name = \"ant_walk.gif\")) ## ~60 seconds\n# setwd(\"~/Intro_R_Workshop/\")\n```\n:::\n\n\n# Basic statistics\n\nIn this quick tutorial we are going to look at how to perform some basic statistical tests. Before we do so, let's remind ourselves how to test the two most common assumptions we must make for any comparison of means test. These are kurtosis and homoscedasticity. Or rather in common parlance, normality of distribution and equality of variance.\n\nWe will use the `laminaria` and `SACTN` data for our examples below in order to practice running the numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpmisc)\nlibrary(pgirmess)\n\n# Load data\nlaminaria <- read_csv(\"../data/laminaria.csv\")\nSACTN <- read_csv(\"../data/SACTN_data.csv\")\n```\n:::\n\n\nWith the libraries and data loaded, we will also create a couple of smaller dataframes from the Laminaria data to make it easier to perform our t-tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Be careful here as we are overwriting our raw data\nlam_1 <- laminaria %>% \n  select(region, site, total_length)\n\n# Create dataframe with a couple of morphometric properties\nlam_2 <- laminaria %>% \n  select(region, site, digits, blade_length)\n```\n:::\n\n\n## Testing assumptions\n\nTo test the normality of the distribution of a set of data we may use the `shapiro.test()` function. This produces a 'w' score as well as a *p*-value, but for now we are only interested in the later. Anything above *p* = 0.05 may considered to be normally distributed.\n\nTo test for similarity of variance we will run the `var()` function. As long as no group of data has \\~4 times greater variance than any other group we are comparing it against it will pass this test.\n\nWith the help of the `%>%` we may test all of our assumptions in one pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First test the Laminaria data by region\n  # This passes our tests\nlam_norm_region <- laminaria %>%\n  group_by(region) %>% \n  summarise(norm_length = shapiro.test(total_length)[2],\n            var_length = var(total_length)) %>% \n  ungroup()\n\n# Then test by site\n  # This no longer passes our tests\nlam_norm_site <- laminaria %>%\n  group_by(site) %>% \n  summarise(norm_length = shapiro.test(total_length)[2],\n            var_length = var(total_length)) %>% \n  ungroup()\n\n# Lastly we test the SACTN data\n  # Which also fails\nSACTN_norm <- SACTN %>% \n  group_by(index) %>% \n  summarise(norm_temp = shapiro.test(temp)[2],\n            var_temp = var(temp, na.rm = T))\n```\n:::\n\n\n## Comparison of two means\n\nTo run a t-test we use `t.test()`. The argument this function wants is in the form of a formula. This requires to bits of information separated by a `~`. On the left we provide the name of the column containing the variable we want to compare between two groups. On the right we put the column containing the grouping variable. The second argument we provide is `data = x`, where we tell R what the name of the dataframe is that contains the columns we have fed to the formula.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(total_length ~ region, data = laminaria)\n```\n:::\n\n\nHappily the Laminaria data, when separated by region, pass our assumption tests. Had they not, we would need to use a Wilcox test instead of a t-test. Note that the arguments are written the exact same for both functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(total_length ~ region, data = laminaria)\n```\n:::\n\n\n## Comparison of multiple means\n\nThe function we use to compare multiple means that pass our assumption tests (parametric data) are `aov()` for an ANOVA and for non-parametric data we use `kruskal.test()` for a Kruskal-Wallis test. To see the difference between the individual factor levels within our multiple means comparisons tests we use `TukeyHSD()` for parametric data and `kruskalmc()` for non-parametric data. Note that `aov()` does not by defalut output the information we are after so we wrap it inside of `summary()`. Note that the Laminaria and SACTN data violate our assumptions. We should therefore not perform paramteric tests on them. We do so below to highlight how these tests work should one have parametric data to use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at the significance results for Laminaria total lengths\nsummary(aov(total_length ~ site, data = laminaria))\n\n# Look at the Tukey test results\nTukeyHSD(aov(total_length ~ site, data = laminaria))\n\n# Multi-level ANOVA\n  # Interaction between factorial levels\nsummary(aov(total_length ~ region * site, data = laminaria))\n\nTukeyHSD(aov(total_length ~ region * site, data = laminaria))\n\n# Single level non-parametric test\nkruskal.test(total_length ~ as.factor(site), data = laminaria)\n\n# Post-test\nkruskalmc(total_length ~ as.factor(site), data = laminaria)\n```\n:::\n\n\n## Correlation\n\nTo check for the correlation between multiple values we may use `cor()`. This may be done in the pipe very quickly, but we have also provided below how to perfomr this test using the base R syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using the pipe\n  # This allows us to compare multple columns of our choosing easily\nlaminaria %>% \n  select(-(region:Ind)) %>% \n  cor(., use = \"complete.obs\")\n\n# Or base R syntax\n  # Here we must specify individual columns\ncor(laminaria$digits, laminaria$blade_length)\n```\n:::\n\n\n## Regression analysis\n\nThe last analysis we will look at in this tut is regression analysis. This is performed by running a linear model, `lm()`, on two columns of data. We do so with the formula notation that we saw earlier but now the righ side of the `~` contains the dependant variable, and the left side the independent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The summary\nsummary(lm(stipe_diameter ~ stipe_mass, data = laminaria))\n\n# Plot the R2 value\nggplot(data = laminaria, aes(x = stipe_mass, y = stipe_diameter)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  stat_poly_eq(formula = y ~ x, \n                aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")), \n                parse = TRUE)\n```\n:::\n\n\n# Data mangling\n\nThis script shows the steps I took to prepare the mangled dataframes used in the tidy data examples in this workshop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load data\nsst_NOAA <- read_csv(\"course_material/data/sst_NOAA.csv\")\n```\n:::\n\n\n## Mangle\n\nAnd now begins the mangling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sites to extract\nsites <- c(\"Med\", \"NW_Atl\", \"WA\")\n\n# Create tidy base\nOISST_tidy <- sst_NOAA %>%\n  mutate(year = year(t)) %>%\n  filter(site %in% sites,\n         year %in% c(2008, 2009)) %>%\n  select(-year)\n\n# First mangle\n  # Normal tidy data\nOISST1 <- OISST_tidy\n\n# Second mangle\nOISST2 <- OISST_tidy %>%\n  pivot_wider(names_from = site, values_from = temp)\n\n# Third mangle\nOISST3 <- OISST_tidy %>%\n  mutate(t = as.character(t),\n         idx = 1:n()) %>% \n  pivot_longer(cols = c(site, t), names_to = \"type\", values_to = \"name\") %>% \n  dplyr::select(idx, type, name, temp)\n\n## Fourth two part mangle\n# A\nOISST4a <- OISST_tidy %>%\n  mutate(t = as.character(t)) %>%\n  unite(index, site, t, sep = \" \")\n\n# B\nOISST4b <- OISST_tidy %>%\n  mutate(t = as.character(t),\n         idx = 1:n()) %>%\n  separate(col = t, into = c(\"year\", \"month\", \"day\"), sep = \"-\") %>%\n  select(-temp)\n```\n:::\n\n\n## Save\n\nHere we save all five of the newly mangled dataframes as one .RData object for ease of loading in the tutorial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave(list = c(\"OISST1\", \"OISST2\", \"OISST3\", \"OISST4a\", \"OISST4b\"), file = \"course_material/data/OISST_mangled.RData\")\n```\n:::\n\n\n# Dates\n\nThis script covers some of the more common issues we may face while dealing with dates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\n\n# Load data\nsad_dates <- read.csv(\"../data/sad_dates.csv\")\n```\n:::\n\n\n## Date details\n\nLook at strip time format for guidance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?strptime\n```\n:::\n\n\nCheck the local time zone\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.timezone(location = TRUE)\n```\n:::\n\n\n## Creating daily dates\n\nCreate date columns out of the mangled date data we have loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create good date column\nnew_dates <- sad_dates %>%\n  mutate(new_good = as.Date(good))\n\n# Correct bad date column\nnew_dates <- new_dates %>%\n  mutate(new_bad = as.Date(bad, format = \"%m/%d/%y\"))\n\n# Correct ugly date column\nnew_dates <- new_dates %>%\n  mutate(new_ugly = seq(as.Date(\"1998-01-13\"), as.Date(\"1998-01-21\"), by = \"day\"))\n```\n:::\n\n\n## Creating hourly dates\n\nIf we want to create date values out of data that have hourly values (or smaller), we must create 'POSIXct' valus because 'Date' values may not have a finer temporal resolution than one day.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correcting good time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_good_hours = as.POSIXct(good_hours, tz = \"Africa/Mbabane\"))\n\n\n# Correcting bad time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_bad_hours = as.POSIXct(bad_hours, format = \"%Y-%m-%d %I:%M:%S %p\", tz = \"Africa/Mbabane\"))\n\n\n# Correcting bad time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_ugly_hours = seq(as.POSIXct(\"1998-01-13 09:00:00\", tz = \"Africa/Mbabane\"),\n                              as.POSIXct(\"1998-01-13 17:00:00\", tz = \"Africa/Mbabane\"), by = \"hour\"))\n```\n:::\n\n\nBut shouldn't there be a function that loads dates correctly?\n\n## Importing dates in one step\n\nWhy yes, yes there is. `read_csv()` is the way to go.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_dates <- read_csv(\"../data/sad_dates.csv\")\n```\n:::\n\n\nBut why does it matter that we correct the values to dates? For starters, it affects the way our plots look/work. Let's create some random numbers for plotting and see how these compare against our date values when we create figures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate random number\nsmart_dates$numbers <- rnorm(9, 2, 10)\n\n# Scatterplot with correct dates\nggplot(smart_dates, aes(x = good, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n# Scatterplot with incorrect dates\nggplot(smart_dates, aes(x = bad, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n# OR\nggplot(smart_dates, aes(x = ugly, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n```\n:::\n\n\nIf the dates are formatted correctly it also allows us to do schnazy things with the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_dates$good[4]+32\nsmart_dates$good[9]-smart_dates$good[3]\nas.Date(smart_dates$good[9]:smart_dates$good[3])\nsmart_dates$good[9]-10247\n```\n:::\n\n\n# GIF\n\nYes my friends, it is true. We may add GIFs to our figures and maps. Rejoice. Better yet, the process is relatively straight forward. We will begin, as usual, by loading our libraries and files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(magick)\n\n# The base image\nbackground <- image_read(\"../data/air_panel.png\") %>% # Load file\n  image_scale(\"900\") # Change resolution\n\n# The gif to overlay\nanim_overlay <- image_read(\"../data/carlton_dance.gif\")  %>% # Load file \n  image_scale(\"300\") # Change resolution\n```\n:::\n\n\n## GIF creation\n\nOnce we have loaded our base image and the GIF we want to put on top of it we need to create a function to make these two different file types 'kiss'. With the appropriately named **`magick`** package this is startlingly easy to do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nframes <- lapply(anim_overlay, function(frame) {\n  image_composite(background, frame, offset = \"+300\")\n})\n```\n:::\n\n\n## GIF animation\n\nWith our function for creating the GIF sorted, it is now time to animate it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanimation <- image_animate(image_join(frames), fps = 10) # FPS = 10 is native speed\n```\n:::\n\n\n## GIF save\n\nJip. Simple as that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage_write(animation, \"../BONUS/carlton.gif\")\n```\n:::\n\n\n# Mapping yourself\n\nYes friends, it is true. Through the magic of the evil empire that is Google, we may indeed map ourselves. Because Google is kind enough to allow us to access our own data, we may see every single thing it records about where we go, what we do, what we say, etc. Isn't that exciting! For today we are only interested in the data Google collects about where we have gone. But don' worry, we aren't going to be zooming in on the data very closely, so the person sitting next to you won't be able to tell if you've been going anywhere particularly naughty.\n\n## **`jsonlite`**\n\nWith new R capabilities comes the requirement for at least one new package. So let's go ahead and install that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n# Package for reading JSON data\n# install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\n# Package for dealing with spatial data\n# install.packages(\"raster\")\nlibrary(raster)\n\n# Packages for changing dates\n# install.packages(\"lubridate\")\nlibrary(lubridate)\n# install.packages(\"zoo\")\nlibrary(zoo)\n\n# Packages for plotting\nlibrary(ggmap)\n\n# A script containing several custom functions\nsource(\"markdown/mapping_yourself_func.R\")\n```\n:::\n\n\nTo download your Google location history please sign in to your Google account (if you aren't already) and then click the following link: <https://takeout.google.com/settings/takeout>. Once you are at the download page please make sure you select *only* \"location history\" for download, otherwise you will be waiting a long time for the download to finish.\n\nThe format of the data you will download is .json. Don't worry about this as we now have the `jsonlite` package to do the hard work for us. It may take your computer a couple of minutes to load your data into R. Some of your files may be quite large if Google has been tracking you more closely...\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note that this file is not in the Intro R Workshop folder\n# You will need to download your own data to follow along\n# I may provide you with my history if you have none\n# location_history <- fromJSON(\"data/LocationHistory.json\")\n# save(location_history, file = \"data/location_history.Rdata\")\nload(\"../data/location_history.RData\")\n```\n:::\n\n\n## Check the data\n\nWith our Google location history data loaded into R we may now start to clean it up so we can create maps and perform analyses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract and clean the locations dataframe\nloc <- location.clean(location_history)\n```\n:::\n\n\nNow that we've cleaned up the data, let's see what we're dealing with.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of times our position was recorded\nloc %>% \n  nrow()\n# The date Google started tracking us\nloc %>%\n  summarise(min(time))\n# The most recent date in Googles memory banks\nloc %>%\n  summarise(max(time))\n```\n:::\n\n\nTo calculate the number of days, months and years of data Google has on us we will use the following code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the number of records per day\npoints_p_day <- loc %>% \n  group_by(date) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"day\")\n\n# Count the number of records per month\npoints_p_month <- loc %>% \n  group_by(month_year) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"month\") %>% \n  rename(date = month_year)\n\n# Count the number of records per year\npoints_p_year <- loc %>% \n  group_by(year) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"year\") %>% \n  rename(date = year)\n\n# Number of days/ months/ years recorded\nnrow(points_p_day)\nnrow(points_p_month)\nnrow(points_p_year)\n```\n:::\n\n\n## Where in the world are you?\n\nIf this hasn't been creepy enough, just wait, there's more! Now we are going to create maps from the data collected on us. Due to the impressive quality of these data there are quite a few sophisticated things we may do with them. We will work through several examples together. The first will be a boxplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First create a dataframe for all of your points of data\n# The [, -1] is removing the 'date' column from each dataframe\npoints <- rbind(points_p_day[,-1], points_p_month[,-1], points_p_year[,-1])\n\n# Now for the figure\nggplot(points, aes(x = group, y = count)) + # The base of the mfigure\n  geom_boxplot(aes(colour = group), size = 1, outlier.colour = NA) + # The boxplot\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) + # Our data points\n  facet_grid(group ~ ., scales = \"free\") + # Facet by day/ month/ year\n    labs(x = \"\", y = \"Number of data points\") + # Change the labels\n  theme(legend.position = \"none\", # Remove the legend\n    strip.background = element_blank(), # Remove strip background\n    strip.text = element_blank()) # Remove strip text\n```\n:::\n\n\nThis shows us how many data points Google tends to collect about us every day, month and year. Why did we plot each boxplot in it's own panel?\n\nUp next we will look at the map of all of these points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we must download the  map of South Africa\n# south_africa <- get_map(location = 'GSouth Africa', zoom = 5)\nload(\"../data/south_africa.RData\")\n\n# Then we may plot our points on it\nggmap(south_africa) + \n  geom_point(data = loc, aes(x = lon, y = lat), \n             alpha = 0.5, colour = \"red\") + \n  labs(x = \"\", y = \"\")\n```\n:::\n\n\nNow let's focus on the Cape Town area specifically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download Cape Town map\n# cape_town <- get_map(location = 'Cape Town', zoom = 12)\nload(\"../data/cape_town.RData\")\n\n# Create the map\nggmap(cape_town) + \n  geom_point(data = loc, aes(x = lon, y = lat), \n             alpha = 0.5, colour = \"khaki\") +\n  labs(x = \"\", y = \"\")\n```\n:::\n\n\nRemember earlier how I said these Google data were very high quality and we could do all sorts of analyses with them? One of the additional things Google tracks is our velocity. So we don't even need to calculate it. We may just plot it as is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame with no NA values for velocity\nloc_2 <- loc %>% \n  na.omit(velocity)\n\nggmap(cape_town) + \n  geom_point(data = loc_2, \n             aes(x = lon, y = lat, colour = velocity), alpha = 0.3) + \n  scale_colour_gradient(low = \"blue\", high = \"red\", \n                        guide = guide_legend(title = \"Velocity\")) +\n  labs(x = \"\", y = \"\")\n```\n:::\n\n\nIf the map above is too zoomed in to see your data try changing the level of the `zoom` argument.\n\n## Big Brother\n\nFor the end of this session we are going to perform two more analyses. The first will be to see how far Google knows that we travel when it is tracking us. And from that we will then understand how Google guesses what it thinks we are doing. Yes, Google's data mining algorithms do think about what you do and record those assumptions. Another service provided by your friendly neighbourhood SkyNet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a distance dataframe\ndistance_p_month <- distance.per.month(loc)\n\n# The distance in KM's Google has tracked you\ndistance_p_month %>% \n  summarise(sum(distance))\n\n# A bar plot of the distances tracked\nggplot(data = distance_p_month, \n       aes(x = month_year, y = distance,  fill = as.factor(month_year))) +\n  geom_bar(stat = \"identity\")  +\n  guides(fill = FALSE) +\n  labs(x = \"\", y = \"Distance (km)\")\n```\n:::\n\n\nLastly, let's take a peek at what it is Google thinks we are doing with ourselves. Because Google records activity probabilities for each tick of its watch, only the activity with highest likelihood at that time is chosen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the activities dataframe\nactivities <- activities.df(location_history)\n\n# The figure\nggplot(data = activities, \n       aes(x = main_activity, group = main_activity, fill = main_activity)) +\n  geom_bar()  +\n  guides(fill = FALSE) +\n  labs( x = \"\", y = \"Count\")\n```\n:::\n\n\n# Morphing\n\nHave you ever wanted to animate the transition from one figure to another? No? Me neither. But hey, it's easy to do, so why not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(magick)\n\n# Load images\nnewlogo <- image_scale(image_read(\"https://www.r-project.org/logo/Rlogo.png\"), \"x150\")\noldlogo <- image_scale(image_read(\"https://developer.r-project.org/Logo/Rlogo-3.png\"), \"x150\")\n```\n:::\n\n\n## Morph creation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmorph_frames <- image_morph(c(oldlogo, newlogo), frames = 100)\n```\n:::\n\n\n## Morph animation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmorph_animate <- image_animate(frames, fps = 20)\n```\n:::\n\n\n## Morph save\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage_write(morph_animate, \"../BONUS/morph.gif\")\n```\n:::\n\n\n# Multivariate stats\n\n> To err is human, but to really foul things up you need a computer.\n>\n> ---*Paul R. Ehrlich*\n\nIn this brief tutorial we are going to walk through the steps necessary to perform a most basic ordination. We will be using MDS for this as it produces, in my opinion, the most straight forward results. There is of course an entire school of thought on this and I, a mere climate scientists, am in no way an authoritative voice on the matter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(vegan)\n\n# Load built-in data\ndata(\"dune\")\ndata(\"dune.env\")\n```\n:::\n\n\n## MDS\n\nMDS, or multi-dimensional scaling, is high level clustering technique. MDS allows us to determine which of the abiotic variables in our dataset are having the most pronounced effects on the clustering of the dunes. Running an MDS on a data frame in R is simple as the `vegan` package will do all of the heavy lifting for us. First we will jump straight in and run an MDS, then we will take a step back and try changing the standardisation of the values and the distance matrix that we would normally need to first calculate. Please consult the help file (`?metaMDS`) for details on the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndune_MDS_1 <- metaMDS(dune)\n```\n:::\n\n\nOr we may be more specific in the way in which we prepare our data for the MDS. Look through the help files to see what other options exist.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardise data\ndune_stand <- decostand(dune, method = \"total\")\n\n# Create Bray-Curtis dissimilarity matrix\ndune_dist <- vegdist(dune_stand, method = \"bray\")\n\n# Create distance matrix\ndune_MDS_2 <- metaMDS(dune_dist)\n```\n:::\n\n\n## Stress\n\nNo, not that stress. We are talking about the stress of the MDS model now. This is an important value to check. If the stress is high (\\>0.3) the MDS model is doing a poor job of modeling the dissimilarities in the data. If it is low (\\<0.1) the model is doing a very good job of displaying the relationships within the data. To check the stress of our results we use the following line of code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Default MDS settings\ndune_MDS_1$stress\n\n# Determined settings\ndune_MDS_2$stress\n```\n:::\n\n\nWhat is the stress of this model? Is that an acceptable level?\n\n## Basic biplot\n\nWith the MDS calculated, and the stress tested, it's time to visualise the first round of results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert for ggplot\ndune_MDS_points <- data.frame(site = 1:nrow(dune)) %>%\n  mutate(x = as.numeric(dune_MDS_2$points[ ,1]),\n         y = as.numeric(dune_MDS_2$points[ ,2]))\n\n# Visualise with ggplot\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  labs(x = \"NMDS1\", y = \"NMDS2\")\n```\n:::\n\n\n## Fitting environmental variables\n\nAs with all of the other ordination analyses we have performed in R thus far, fitting environmental variables may also be done with one easy step. We do this by providing the `envfit()` function with a formula, the same as we do for linear models. The dependent variable (to the left of the `~`) will be the results of the MDS on the species assemblage data, and the independent variables (to the right of the `~`) are the columns from our environmental variables data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndune_envfit <- envfit(dune_MDS_2 ~ Moisture + Use, data = dune.env)\ndune_envfit\n```\n:::\n\n\nIn the printout above we see the results for the R\\^2 (here r2) and *p*-values for the fit of each abiotic variable to the species assemblage data. Which relationships are significant? Which variable(s) appears to best explain the variance in the species assemblages? Which of the axes of the MDS have the strongest relationship with which variable?\n\nTo plot the results of our fitted abiotic variables on top of our species MDS we need to quickly prep it to play nice with **`ggplot2`** and then we need only append a couple of lines onto the chunk we wrote to display our MDS results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the envfit vector values\ndune_envfit_df <- data.frame(dune_envfit$factors$centroids) %>%\n  mutate(factors = row.names(.)) %>%\n  rename(x = NMDS1, y = NMDS2)\n\n# Visualise environmental fits\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  labs(x = \"NMDS1\", y = \"NMDS2\")\n```\n:::\n\n\n## Adding clusters\n\nIn order to add clustering we must first create groupings for our data. In this instance we will be calculating our groups using hierarchical cluster analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create dendrogram\n  # Note that this must be run on a distance matrix\ndune_clust <- hclust(dune_dist, \"ward.D\")\n\n# Extract clusters\n  # In this case we have decided on four clusters\ndune_grp <- cutree(dune_clust, 4)\n\n# Extract groups for plotting\ndune_MDS_points <- dune_MDS_points %>% \n  mutate(grp_id = as.factor(dune_grp))\n```\n:::\n\n\nWith the clusters calculated we may now plot ellipses on our biplot. We will first do this with the built-in functionality of **`ggplot2`**, which unfortunately isn't great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  # The ellipses\n  stat_ellipse(aes(colour = grp_id), type = \"t\") + \n  #\n  labs(x = \"NMDS1\", y = \"NMDS2\", colour = \"Cluster\")\n```\n:::\n\n\nIf we have very large datasets the ellipses will come more in line with what we want. With small datasets not so much. This is because the ellipses are actually calculating the area under which a certain confidence interval is maintained that the points in that group may be found. If we would rather use polygons to fit directly onto the area of our clusters we do so by replacing the ellipses with the following line of code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  # The custom made polygons\n  stat_chull(geom = \"polygon\", aes(fill = grp_id), alpha = 0.4) +\n  #\n  labs(x = \"NMDS1\", y = \"NMDS2\")\n```\n:::\n\n\nI'm not super excited about that result either. A third option is to simply change the colour of the points to reflect their grouping.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  # Changing point aesthetics\n  geom_point(size = 8, aes(colour = grp_id)) +\n  #\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, \n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\",\n            aes(label = factors)) +\n  labs(x = \"NMDS1\", y = \"NMDS2\", colour = \"Cluster\")\n```\n:::\n\n\nI think this is actually the cleanest way to visualise the data.\n\n## Diversity\n\nIf we are interested in calculating a Shannon-Wiener index on the species diversity found within the dunes we need only one function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiversity(dune)\n```\n:::\n\n\n## ANOSIM\n\nOne final thing. It is also necessary to know if any differences exist between the clusters we have determined for our data. To do this we use the `anosim()` function from the `vegan` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanosim(dune_dist, dune_grp)\n```\n:::\n\n\n# R Markdown\n\nThe workshop pdf we have been using for the last three days is actually produced from R Markdown and you may view the R Markdown document (the .Rmd file) to see all of the code that created this pdf in its native state. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. R Markdown is based on the language Markdown, which is another computer language, somewhere in between \\LaTeX and Microsoft Word. R Markdown differs from Markdown in that it is also able to understand the R code we give it. Furthermore, RStudio has built into it the capabilities necessary to use R Markdown 'out of the box'. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. In order to use this tutorial effectively, please examine the R Markdown file ('Intro_R\\_2017.Rmd') in conjunction with the output document ('Intro_R\\_2017.pdf'). If you double click on the 'Intro_R\\_2017.Rmd' file it will open in the RStudio editor.\n\n## Quick examples\n\nBelow is just a quick overview of the many common things one will need to know to put an RMarkdown document together.\n\n### Text\n\nThis is text in *italics*.\n\nAnd this text is in **bold**.\n\nThis text is in `code font`.\n\nYou can embed an R code chunk like this and show it and the data it produces:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggmap)\n# load(\"../data/cape_point_sites.Rdata\")\n# cape_point <- get_map(location = c(lon = 18.36519, lat = -34.2352581),\n#                         zoom = 11, maptype = 'roadmap')\n# load(\"../data/cape_point.Rdata\")\n# str(cape_point)\n\n  # *   site name --- `r cape_point_sites[1,1]`\n  # *   longitude --- `r cape_point_sites[1,2]`\n  # *   latitude --- `r cape_point_sites[1,3]`\n```\n:::\n\n\nYou can also embed R output directly into sentences as in this example:\n\nSome site details are in this list:\n\n### Tables\n\nThere are many ways to produce tables in R Markdown. A short search will provide many alternatives. The **`xtable`** package is another excellent choice as this provides even more options for how your table output will appear. Here we provide one example:\n\n\n\n```{.r .cell-code}\nknitr::kable(\n  head(mtcars[, 1:8], 10), booktabs = TRUE,\n  caption = 'A table of the first 10 rows of the mtcars data.'\n)\n```\n\n\nTry looking up the help file for `?kable()` to learn more about what may be done with this function.\n\n### Images\n\nImages stored on your computer, such as \\ref{fig:accurate}, can be embedded in your document and even cross referenced.\n\n\n::: {.cell layout-align=\"centre\"}\n\n:::\n\n\nNotice that in order to display images in this way we need to make sure R knows we are using the **`knitr`** package function `include_graphics()`, which allow one to display images of all sorts of file types without any fuss.\n\nYou can also embed any plots produced by R, for example:\n\n\n::: {.cell}\n\n:::\n\n\nNotice above how the first line specifying the start of the R code includes some specifications with regards to the size of the figure, its caption, etc. Note too that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. The options `results = TRUE`, `message = TRUE` and `warning = TRUE` have similar functions. Info on the other code chunk options can be found at the [R Markdown website](http://rmarkdown.rstudio.com/authoring_rcodechunks.html) or in the Cheatsheets and other documentation accessible via the RStudio 'Help' in the menu bar.\n\n> HTML hyperlinks\\\n> See how the code above also demonstrates how to embed links to external websites.\n\n### References\n\nWe can also have some references... This document was made using the R software [@R2017] and various add-on packages [@vegan2017]. The *vegan* package was produced by @vegan2017 some time ago.\n\n## Creating a document\n\nEven though we may immediately begin authoring documents with RStudio, we are limited to .html and .doc file types. If we want to author .pdf files, such as the one we are reading now, we must install 'LaTeX' on our computers. This installation process is beyond the scope of this course but there are many resources available online to aid one in the process and the software is, of course, free.\n\nWhether or not you have 'LaTeX' installed, when you click the **Knit** button (with the option to create multiple document kinds) a document will be generated that includes both content as well as the output of any embedded R code chunks (portions of R code surrounded by code that denotes the R commands) within the document. R code chunks can be used to render R output into documents or to simply display code for illustration, as outlined above.\n\nThis is a terribly basic demonstration, but since beautiful documentation already exists I suggest you go and find the necessary examples on the R Markdown website indicated above for a more in-depth account of how to use it.\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}