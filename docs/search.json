[
  {
    "objectID": "wild.html",
    "href": "wild.html",
    "title": "Wild Data",
    "section": "",
    "text": "When we start out with the processes of finding data ‘in the wild’ and sourcing them it can feel very overwhelming. With a few considerations however we can drastically cut down on the feelings of uncertainty that enshroud this process. For starters, the vast majority of the environmental and biological data that we may want (on a global scale, local scale can be a bit more specific) can be found in just a handful of places. The methods for accessing these data are also not as varied as they may first appear. The two presentations+exercises in this unit will cover 1) where we may find the majority of the wild data we may need, and 2) the most common methods of accessing those data."
  },
  {
    "objectID": "wild.html#slides-and-application-exercises",
    "href": "wild.html#slides-and-application-exercises",
    "title": "Wild Data",
    "section": "Slides and application exercises",
    "text": "Slides and application exercises\n\nWild 1: Where data roam free\n\nSlides\n\n\nSource\n\n\n\nHome on the range\n\nSource\n\n\n\nWild 2: The local data shop\n\nSlides\n\n\nSource\n\n\n\nOrders up!\n\nSource"
  },
  {
    "objectID": "wild.html#diy-wild-data",
    "href": "wild.html#diy-wild-data",
    "title": "Wild Data",
    "section": "DIY wild data",
    "text": "DIY wild data\nAt the end of this unit we should now be equipped with the tools we need to source, download, tidy, analyse, and visualise data from a number of free data sources out in the wild. For this DIY session let’s select a dataset out in the wild that has particular importance to the work that we do. But rather than downloading it via a web interface, let’s write a script that contains the full pipeline that can, with the push of one button, download, tidy, analyse, and visualise the data. Saving a faceted/combined figure to our local computer. Preferably with a map, if that would make sense given the dataset in question."
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Workflows in R",
    "section": "",
    "text": "Learning to perform analyses in R and create figures is great, but without a useful workflow we won’t get far. The ideal is that we can develop good habits around which we can organise and optimise our workflows. To that end, this page lays out some thoughts on the philosophy of workflows in R. It is broken into three parts that are designed to be consider sequentially, one day at a time. The writing here is much more verbose than the rest of the workshop because this page is also designed to be a reference for any workflow questions that attendees may have after the workshop concludes."
  },
  {
    "objectID": "workflow.html#r-workflow---i",
    "href": "workflow.html#r-workflow---i",
    "title": "Workflows in R",
    "section": "R Workflow - I",
    "text": "R Workflow - I\n\nStyle and code conventions\nEarly on, develop the habit of unambiguous and consistent style and formatting when writing your code, or anything else for that matter. Pay attention to detail and be pedantic, as with scientific writing in general. Although many R commands rely on precisely formatted statements (code blocks), style can nevertheless to some extent have a personal flavour to it. The key is consistency. One may notice that the content for this workshop follows a certain convention to improve readability.\n\nPackage names are shown in a bold font over a grey box, e.g. tidyr.\nFunctions are shown in normal font followed by parentheses and also over a grey box , e.g. plot(), or summary().\nOther R objects, such as data, function arguments or variable names are again in normal font over a grey box, but without parentheses, e.g. x and apples.\nSometimes a package that contains a specific function is referenced using two colons, e.g. dplyr::filter().\nCommands entered into the R command line (console) and the output that it returns will be shown in a code block, which is a light grey background with coloured code font.\n\nConsult these resources for more about R code style :\n\nGoogle’s R style guide\nThe tidyverse style guide\nHadley Wickham’s advanced R style guide\n\n\n\nHelp\nThe help files in R are not always clear. It requires a bit of work to understand them well. There is method however to what appears to be madness. Please type ?read.table() in your console now to bring up this help file in your RStudio GUI.\nThe first thing we see at the top of the help file in small font is the name of the function, and the package it comes from in curly braces. After this, in very large text, is a very short description of what the function is used for. After this is the ‘Description’ section, which gives a sentence or two more fully explaining the use(s) of the function. The ‘Usage’ then shows all of the arguments that may be given to the function, and what their default settings are. When we write a function in our script we do not need to include all of the possible arguments. The help file shows us all of them so that we know what our options are. In some cases a help file will show the usage of several different functions together. This is done, as is the case here, if these functions form a sort of ‘family’ and share many common purposes. The ‘Arguments’ section gives a long explanation for what each individual argument may do. The Arguments section here is particularly verbose. Up next is the ‘Details’ section that gives a more in depth description of what the function does. The ‘Value’ section tells us what sort of output we may expect from the function. Some of the more well documented functions, such as this one, will have additional sections that are not a requirement for function documentation. In this case the ‘Memory usage’ and ‘Note’ sections are not things one should always expect to see in help files. Also not always present is a ‘References’ section. Should there be actual published documentation for the function, or the function has been used in a publication for some other purpose, these references tend to be listed here. There are many functions in the vegan package that have been used in dozens of publications. If there is additional reading relevant to the function in question, the authors may also have included a ‘See also’ section, but this is not standard. Lastly, any well documented function should end with an ‘Examples’ section. The code in this section is designed to be able to be copy-pasted directly from the help file into the users R script or console and run as is. It is perhaps a bad habit, but when I am looking up a help file for a function, I tend to look first at the Examples section. And only if I can’t solve my problem with the examples do I actually read the documentation.\n\n\nR Scripts\nThe first step for any project in R is to create a new script. We do this by clicking on the ‘New Document’ button (in the top left and selecting ‘R Script’). This creates an unnamed file in the Source Editor pane. Best to save it first of all so we do not lose what we do. ‘File’/‘Save As’/ and the Working Directory should come up. Type in workflow as the file name and click ‘save.’ R will automatically add a .R extension.\nIt is recommended to start a script with some basic information for you to refer back to later. Start with a comment line (the line begins with a #) that tells you the name of the script, what its purpose is, what it does, who created it, and the date it was created. In the source editor enter to following lines and save the file again:\n\n# workflow.R\n# <purpose of script>\n# <what it does>\n# <your name>\n# <current date>\n\nRemember that anything appearing after the # is not executed by R as script and is a comment.\nIt is recommended that for the DIY sessions at the end of each day you start a new script (in the Source Editor). That way you will have a record of what you have done.\nBelow we will see how to import the file sst_NOAA.csv into R, assign it to a dataframe named sst_NOAA, and spend a while looking it over. These data contain the daily sea surface temperature (SST) values from 1982-2021 at three locations that experienced, at some point over that period, a well studied marine heatwave (MHW). The name of the location is given in the site column, the daily dates in t, and the SST values in temp.\n\nComments\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.\n\n\n\nReading data into R\nR will read in many types of data, including spreadsheets, text files, binary files and files from other statistical packages and software.\n\nFull stops\nIn most of the world we are taught from a young age to use commas (,) instead of full stops (.) for decimal places. This simply will not do when we are working with a computer. You must always use a full stop for a decimal place and never insert commas anywhere into any numbers.\n\n\nCommas\nR generally thinks that commas mean the user is telling the computer to separate values. So if you think you are typing a big number like 2,300 you may actually end up with two numbers. Never use commas with numbers.\n\n\nPreparing data for R\nImporting data can sometimes take longer than the statistical analysis itself! In order to avoid as much frustration as possible it is important to remember that for R to be able to comprehend your data they need to be in a consistent format, with each variable in a column and each sample in a row. The format within each variable (column) needs to be consistent and is commonly one of the following types: a continuous numeric variable (e.g., fish length (m): 0.133, 0.145); a factor or categorical variable (e.g., Month: Jan, Feb or 1, 2, …, 12); a nominal variable (e.g., algal colour: red, green, brown); or a logical variable (i.e., TRUE or FALSE). You can also use other more specific formats such as dates and times, and more general text formats.\nWe learn more about working with data in R in the Tidy Data (Day 5) slides and exercises. Including the distinction between long and wide format data. For most of our work in R we require our data to be in the long format, but Excel users tend to be more familiar with data stored in the wide format. For now let’s bring some data into R and not worry too much about the data being tidy.\n\n\nConverting data\nBefore we can read in the sst_NOAA dataset provided for the following exercises, we need to convert the Excel file (i.e. .xlsx) supplied into a .csv file. Open sst_NOAA.xlsx in Excel, then select ‘Save As’ from the File menu. In the ‘Format’ drop-down menu, select the option called ‘Comma Separated Values’, then hit ‘Save’. You’ll get a warning that formatting will be removed and that only one sheet will be exported; simply ‘Continue’. Your working directory should now contain a file called sst_NOAA.csv.\n\n\nImporting data\nThe easiest way to import data into R is by changing your working directory to be the same as the file path where the file(s) are you want to load. A file path is effectively an address. In most operating systems, if you open the folder where your files are you may click on the navigation bar and it will show you the complete file path. Many people develop the nasty habit of squirling away their files within folders within folders within folders within folders… within folders within folders. Please don’t do that.\nThe concept of file paths is either one that you are familiar with, or you’ve never heard of before. There tends to be little middle ground. Happily, RStudio allows us to circumvent this issue. We do this by using the R_workshop.Rproj that you may find in the files downloaded for this workshop. If you have not already switched to the R_Workshop.Rproj as outlined in the RStudio primer, click on the project button in the top right corner your RStudio window. Then navigate to where you saved R_Workshop.Rproj and select it. Notice that your RStudio has changed a bit and all of the objects you may have previously created in your environment have been removed and any tabs in the source editor pane have been closed. That is fine for now, but it may mean you need to re-open the workflow.R script you just created.\nOnce we have the working directory set, either by doing it manually with setwd() or by loading a project, R will now know where to look for the files we want to read. The function read_csv() is the most convenient way to read in raw data. There are several other ways to read in data, but for the purposes of this workshop we’ll stick to this one, for now. To find out what it does, we will go to its help entry in the usual way (i.e. ?read_csv).\n\nData formats\nR has pedantic requirements for naming variables. It is safest to not use spaces, special characters (e.g., commas, semicolons, any of the shift characters above the numbers), or function names (e.g., mean). One can use ‘camelCase’, such as myFirstVariable, or snake case, such as my_first_variable. Always make sure to use meaningful names; eventually you will learn to find a balance between meaningfulness and something short that’s easy enough to retype repeatedly (although R’s ability to use tab completion helps with not having to type long names to often).\n\n\nImport\nread_csv() is simply a ‘wrapper’ (i.e., a command that modifies) a more basic command called read_delim(), which itself allows you to read in many types of files besides .csv. To find out more, type ?read_delim().\n\n\n\nLoading a file\nTo load the sst_NOAA.csv file we created, and assign it to an object name in R, we will use the read_csv() function from the tidyverse package, so let’s make sure it is activated.\n\nlibrary(tidyverse)\n\nDepending on the version of Excel you are using, or perhaps the settings within it, the sst_NOAA.csv file you created may be changed in different ways without being notified. Generally Excel likes to replace the , between columns in our .csv files with ;. This may seem like a triviality but sadly it is not. Lucky for use, the tidyverse knows about this problem and they have made a plan. Please open your sst_NOAA.csv file in a text editor (e.g. notepad) and look at which character is being used to separate columns. read_csv() has become clever enough over the years that it now understands what the delimiter should be. But do be careful here.\n\n# Note what the 'Delimiter' is \nsst_NOAA <- read_csv(\"course_material/data/sst_NOAA.csv\")\n\nIf one clicks on the newly created sst_NOAA object in the Environment pane it will open a new panel that shows the information as a spreadsheet. To go back to your script click the appropriate tab in the Source Editor pane. With these data loaded we may now perform analyses on them.\nAt any point when working in R, you can see exactly what objects are in memory in several ways. First, you can look at the Environment tab in RStudio, then Workspace Browser. Alternatively you can type either of the following:\n\nls()\n# or\nobjects()\n\nYou can delete an object from memory by specifying the rm() function with the name of the object:\n\nrm(sst_NOAA)\n\nThis will of course delete our dataset, so we may import it again with the same line of code as necessary.\n\nsst_NOAA <- read_csv(\"course_material/data/sst_NOAA.csv\")\n\n\nManaging variables\nIt is good practice to remove variables from memory that you are not using, especially if they are large.\n\n\n\n\nExamine your data\nOnce the data are in R, you need to check there are no glaring errors. It is useful to call up the first few lines of the dataframe using the function head(). Try it yourself by typing:\n\nhead(sst_NOAA)\n\nThis lists the first six lines of each of the variables in the dataframe as a table. You can similarly retrieve the last six lines of a dataframe by an identical call to the function tail(). Of course, this works better when you have fewer than 10 or so variables (columns); for larger data sets, things can get a little messy. If you want more or fewer rows in your head or tail, tell R how many rows it is you want by adding this information to your function call. Try typing:\n\nhead(sst_NOAA, n = 3)\ntail(sst_NOAA, n = 2)\n\nYou can also check the structure of your data by using the glimpse() function:\n\nglimpse(sst_NOAA)\n\nThis very handy function lists the variables in your dataframe by name, tells you what sorts of data are contained in each variable (e.g., continuous number, discrete factor) and provides an indication of the actual contents of each.\nIf we wanted only the names of the variables (columns) in the dataframe, we could use:\n\nnames(sst_NOAA)"
  },
  {
    "objectID": "workflow.html#r-workflow---ii",
    "href": "workflow.html#r-workflow---ii",
    "title": "Workflows in R",
    "section": "R Workflow - II",
    "text": "R Workflow - II\nIf you are starting at this point, rather than continuing directly from above, please load the workflow.R script we created during ‘R Workflow - I’.\n\nTidyverse sneak peek\nBefore we begin to manipulate our data further we need to briefly introduce ourselves to the tidyverse. And no introduction can be complete within learning about the pipe command, %>%. We may type this by pushing the following keys together: ctrl+shift+m. The pipe (%>%) allows us to perform calculations sequentially, which helps us to avoid making errors.\nThe pipe works best in tandem with the following five common functions:\n\nArrange observations (rows) with arrange()\n\nSelect variables (columns) withselect()\n\nFilter observations (rows) with filter()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\nWe will cover these functions in more detail on Day 5. For now we will ease ourselves into the code with some simple examples.\n\n\nSubsetting\nNow let’s have a look at specific parts of the data. You will likely need to do this in almost every script you write. If we want to refer to a variable, we specify the dataframe then the column name within the select() function. In your script type:\n\nsst_NOAA %>% # Tell R which dataframe we are using\n  select(site, temp) %>% # Select only specific columns\n  head(3) # Just here to limit the printout on this page...\n\nIf we want to only select values from specific columns and rows we insert one more line of code.\n\nsst_NOAA %>% \n  select(site, temp) %>% # Select specific columns first\n  slice(56:58)\n# what does the '56:58' do? Change some numbers and run the code again. What happens?\n\nIf we wanted to select only the rows of data belonging to the Mediterranean site, we could type:\n\nsst_NOAA %>%\n  filter(site == \"Med\") %>% \n  head(3)\n\nThe function filter() has two arguments: the first is a dataframe (we specify sst_NOAA in the previous line and the pipe supplies this for us) and the second is an expression that relates to which rows of a particular variable we want to include. Here we include all rows for Med and we find that in the variable site. It returns a subset that is itself a dataframe in the same form as the original dataframe. We could assign that subset of the full dataframe to a new dataframe if we wanted to.\n\nsst_NOAA_med <- sst_NOAA %>% \n  filter(site == \"Med\")\n\n\nDIY: Subsetting\nIn the script you have started, create a new named dataframe containing only SST from two of the sites. Check that the new dataframe has the correct values in it. What purpose can the naming of a newly-created dataframe serve?\n\n\n\nBasic stats\nStraight out of the box it is possible in R to perform a broad range of statistical calculations on a dataframe. If we wanted to know how many daily samples we have in the Med, we simply type the following:\n\nsst_NOAA %>% # Tell R which dataset to use\n  filter(site == \"Med\") %>% # Filter out only records from the Med\n  nrow() # Count the number of remaining rows\n\nOr, if we want to select only the row with the highest temperature:\n\nsst_NOAA %>% # Tell R which dataset to use\n  filter(temp == max(temp)) # Select row with max total length\n\nNow exit RStudio. Pretend it is three days later and revisit your analysis. Calculate the number of entries at Med and find the row with the highest temperature.\nImagine doing this daily as our analysis grows in complexity. It will very soon become quite repetitive if each day you had to retype all these lines of code. And now, six weeks into the research and attendant statistical analysis, you discover that there were some mistakes and some of the raw data were incorrect. Now everything would have to be repeated by retyping it at the command prompt. Or worse still (and bad for repetitive strain injury) doing all of it in SPSS and remembering which buttons to click and then re-clicking them. A pain. Let’s avoid that altogether and do it the right way by writing an R script to automate and annotate all of this.\n\nDealing with missing data\nThe .csv file format is usually the most robust for reading data into R. Where you have missing data (blanks), the .csv format separates these by commas. However, there can be problems with blanks if you read in a space-delimited format file. If you are having trouble reading in missing data as blanks, try replacing them in your spreadsheet with NA, which is the code for missing data in R. In Excel, highlight the area of the spreadsheet that includes all the cells you need to fill with NA. Do an Edit/Replace… and leave the ‘Find what:’ textbox blank and in the ‘Replace with:’ textbox enter NA, the missing value code. Once imported into R, the NA values will be recognised as missing data.\n\nRemember that in an R script, you can run individual lines of code by highlighting them and pressing ctrl-Enter (cmd-Enter on a Mac). Your R script should now look similar to this one, but of course you will have added your own notes and comments as you went along:\n\n# workflow.R\n# <What is the purpose>\n# <What it does>\n# <name>\n# <date>\n\n# Find the current working directory (it will be correct if a project was\n# created as instructed earlier)\ngetwd()\n\n# If the directory is wrong because you chose not to use an Rworkspace (project),\n# set your directory manually to where the script will be saved and where the data\n# are located\n# setwd(\"<insert_path_here>\")\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load the data\nsst_NOAA <- read_csv(\"course_material/data/sst_NOAA.csv\")\n\n# Examine the data\nhead(sst_NOAA, 5) # First five lines\ntail(sst_NOAA, 2) # Last two lines\nglimpse(sst_NOAA) # A more thorough summary\nnames(sst_NOAA) # The names of the columns\n\n# Subsetting data\nsst_NOAA %>% # Tell R which dataframe to use\n  select(site, temp) %>% # Select specific columns\n  slice(56:78) # Select specific rows\n\n# How many data points do we have in the Med?\nsst_NOAA %>%\n  filter(site == \"Med\") %>%\n  nrow()\n\n# The row with the highest temperature\nsst_NOAA %>% # Tell R which dataset to use\n  filter(temp == max(temp)) # Select row with max total length\n\nMaking sure all the latest edits in your R script have been saved, close your R session. Pretend this is now two years in the future and you need to revisit the analysis. Open the file you created in 2022 in RStudio. All you need to do now is highlight the file’s entire contents and hit ctrl-Enter.\n\nStick with .csv files\nThere are packages in R to read Excel spreadsheets (e.g., .xlsx), but remember there are likely to be problems reading in formulae, graphs, macros and multiple worksheets. We recommend exporting data deliberately to .csv files (which are also commonly used in other programs). This not only avoids complications, but also allows you to unambiguously identify the data you based your analysis on. This last statement should give you the hint that it is good practice to name your .csv slightly differently each time you export it from Excel, perhaps by appending a reference to the date it was exported.\n\n\nRemember…\nFriends don’t let friends use Excel.\n\n\n\nSummary of all variables in a dataframe\nOnce we’re happy that we know what the variables are called and what sorts of data they contain, we can dig a little deeper. Try typing:\n\nsummary(sst_NOAA)\n\nThe output is quite informative. It tabulates variables by name, and for each provides summary statistics. For continuous variables, the name, minimum, maximum, first, second (median) and third quartiles, and the mean are provided. For factors (categorical variables), a list of the levels of the factor and the count of each level are given. In either case, the last line of the table indicates how many NAs are contained in the variable. The function summary() is useful to remember as it can be applied to many different R objects (e.g., variables, dataframes, models, arrays, etc.) and will give you a summary of that object. We will use it liberally throughout the workshop.\n\n\nSummary statistics by variable\nThis is all very convenient, but we may want to ask R specifically for just the mean of a particular variable. In this case, we simply need to tell R which summary statistic we are interested in, and to specify the variable to apply it to using summarise(). Try typing:\n\nsst_NOAA %>% # Chose the dataframe\n  summarise(mean_temp = mean(temp)) # Calculate mean temperature\n\nOr, if we wanted to know the mean and standard deviation for the temperature across all sites, do:\n\nsst_NOAA %>% # Tell R that we want to use the 'sst_NOAA' dataframe\n  group_by(site) %>%  # Tell R to perform the following calculations on groups\n  summarise(mean_temp = mean(temp), # Create a summary of the mean of temperature\n            sd_temp = sd(temp)) # Create a summary of the SD of the temperature\n\nOf course, the mean and standard deviation are not the only summary statistic that R can calculate. Try max(), min(), median(), range(), sd() and var(). Do they return the values you expected?\n\n\nMore complex calculations\nLet’s say you want to calculate something that is not standard in R (e.g. the standard error of the mean for a variable). How can this be done?\nThe trick is to remember that R is a calculator, so we can use it to do maths, even complex maths (which we won’t do). We know that the variance is given by var(), so all we need to do is figure out how to get n and calculate a square root. The simplest way to determine the number of elements in a variable is a call to the function n(). We may therefore calculate standard error with one chunk of code, step by step, using the pipe. Furthermore, by using group_by() we may calculate the standard error for all sites in one go.\n\nsst_NOAA %>% # Select 'laminaria'\n  group_by(site) %>% # Group the dataframe by site\n  summarise(var_temp = var(temp), # Calculate variance\n            n_temp = n()) %>%  # Count number of values\n  mutate(se_temp = sqrt(var_temp / n_temp)) # Calculate se\n\n\n\nCode chunks\nNot only does keeping our code grouped in ‘chunks’ keep our workflow tidier, it also makes it easier to read for ourselves, our colleagues, and most importantly, our future selves. When we look at the previous code chunk we can think of it as a paragraph in a research report, with each line a sentence. If I were to interpret this chunk of code in plain English it would sound something like this:\n\nI start by taking the original sst_NOAA data. I then grouped the data into different sites. After this I calculated the mean temperature for each site, as well as counting the number of observations within each site. FInally, I caluclate the standard error by finding the square root of the variance over the number of samples.\n\nJust like paragraphs in a human language may vary in length, so too may code chunks. There really is no limit. This is not to say that it is encouraged to attempt to reproduce a code chunk of comparable length to anything Marcel Proust would have written. It is helpful to break things up into pieces of a certain size. The best size is up to the discretion of the person writing the code. It is up to you to find out for yourself what works best for you.\n\n\nMissing values (NA)\nSometimes, you need to tell R how you want it to deal with missing data. In the case that you have NA in the named variable, R takes the cautious approach of giving you the answer of NA, meaning that there are missing values here. This may not seem useful, but as the programmer, you can tell R to respond differently, and it will. Simply append an argument to your function call, and you will get a different response. For example:\n\nsst_NOAA %>% \n  summarise(mean_temp = mean(temp, na.rm = T))\n\nThe na.rm argument tells R to remove (or more correctly ‘strip’) NA values from the data string before calculating the mean. Although needing to deal explicitly with missing values in this way can be a bit painful, it does make you more aware of missing data, what the analyses in R are doing, and makes you decide explicitly how you will treat missing data.\nWhen calculating the mean, we specified that R should strip the NA values, using the argument na.rm = TRUE. In the example above, we didn’t have NA values in the variable of interest. What happens if we do?\nUnfortunately, the call to the function n() has no arguments telling R how to treat NA values; instead, they are simply treated as elements of the variable and are therefore counted. The easiest way to resolve this problem is to strip out NA values in advance of any calculations.\n\nsst_NOAA %>% \n  select(temp) %>% \n  na.omit() %>% \n  summarise(n = n())\n\nWere there missing values in this dataset, the function na.omit() would remove them from the variable that is specified as its argument.\n\n\nSaving data\nA major advantage of R over many other statistics packages is that you can generate exactly the same answers time and time again by simply re-running saved code. However, there are times when you will want to output data to a file that can be read by a spreadsheet program such as Excel (but try not to… please). The simplest general format is .csv (comma-separated values). This format is easily read by Excel, and also by many other software programs. To output a .csv type:\n\nwrite_csv(sst_NOAA_med, path = \"course_material/data/sst_NOAA_med.csv\")\n\nThe first argument is simply the name of an object in R, in this case our table (a data object of class table) of SST for the Mediterranean (other sorts of data are available, so play around to see what can be done). The second argument is the name of the file you want to write to. This file will always be written to your working directory, unless otherwise specified by including a different path in the file name. Remember that file names need to be within quotation marks.\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems. Scripts execute sequentially from top to bottom. Try and work out what each line of the program is doing and discuss it with your neighbour. Note, if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio.\n\n\nClearing the memory\nYou will be left with many objects after working through these examples. Note that in RStudio when you quit it can save the Environment if you choose, and so it can retain the objects in memory when you start RStudio again. The choice to save the objects resulting from an R Session until next time can be selected in the Global Options menu (‘Tools’ > ‘Global Options’ > ‘General’ > ‘Save workspace to .RData on exit’). Personally, we never save objects as it is preferable to start on a clean slate when one opens RStudio. Either way, to avoid long load times and clogged memory, it is good practice to clear the objects in memory every now and then unless you can think of a compelling reason not to. This may be done by clicking on the broom icon at the top of the Environment pane.\nOf course, you could remove an individual object by placing only its name within the brackets of rm(). Do not use this line of code carelessly in the middle of your script; doing so will mean that you have to go back and regenerate the objects you accidentally removed – this is more of a nuisance than a train smash, especially for long, complicated scripts, as you will have (I hope!) saved the R script from which the objects in memory can be regenerated at any time.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved)."
  },
  {
    "objectID": "workflow.html#r-workflow---iii",
    "href": "workflow.html#r-workflow---iii",
    "title": "Workflows in R",
    "section": "R Workflow - III",
    "text": "R Workflow - III\nIf you are starting at this point, rather than continuing directly from above, please load the workflow.R script we created during ‘R Workflow - I’ and ‘R Workflow - II’.\n\nAdditional useful functions\nThere is an avalanche of useful functions to be found within the tidyverse. In truth, we have only looked at functions from three packages: ggplot2, dplyr, and tidyr. There are far, far too many functions even within these three packages to cover within a week. But that does not mean that the functions in other packages, such as purrr are not also massively useful for our work. More on that tomorrow (Day 6). For now we will see how the inclusion of a handful of choice extra functions may help to make our workflow even tidier.\n\nRename variables (columns) with rename()\nWe have seen that we select columns in a dataframe with select(), but if we want to rename columns we have to use, you guessed it, rename(). This functions works by first telling R the new name you would like, and then the existing name of the column to be changed. This is perhaps a bit back to front, but such is life on occasion.\n\nsst_NOAA %>% \n  rename(source = site) %>% \n  head(3)\n\n\n\nCreate a new dataframe for a newly created variable (column) with transmute()\nIf for whatever reason one wanted to create a new variable (column), as one would do with mutate(), but one does not want to keep the dataframe from which the new column was created, the function to use is transmute().\n\nsst_NOAA %>% \n  transmute(kelvin = temp + 273.15) %>% \n  head(3)\n\nThis makes a bit more sense when paired with group_by() as it will pull over the grouping variables into the new dataframe. Note that when it does this for us automatically it will provide a message in the console.\n\nsst_NOAA %>% \n  group_by(site, t) %>% \n  transmute(kelvin = temp + 273.15) %>% \n  head(3)\n\n\n\nCount observations (rows) with n()\nWe have already seen this function sneak it’s way into a few of the code chunks in the previous session. We use n() to count any grouped variable automatically. It is not able to be given any arguments, so we must organise our dataframe in order to satisfy it’s needs. It is the diva function of the tidyverse; however, it is terribly useful as we usually want to know how many observations our summary stats are based on. First we will run some stats and create a figure without n. Then we will include n and see how that changes our conclusions.\n\n sst_NOAA_n <- sst_NOAA %>%\n  mutate(month = lubridate::month(t)) %>%\n  group_by(site, month) %>%\n  summarise(mean_temp = round(mean(temp, na.rm = T)),\n            .groups = \"drop\") %>%\n  arrange(mean_temp) %>%\n  select(mean_temp) %>%\n  distinct()\n\nggplot(data = sst_NOAA_n, aes(x = 1:nrow(sst_NOAA_n), y = mean_temp)) +\n  geom_point() +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\nThat looks vaguely linear… To make things more interesting, now let’s change the size of the dots to show how frequently each of these mean temperatures is occurring.\n\n sst_NOAA_n <- sst_NOAA %>%\n  mutate(month = lubridate::month(t)) %>%\n  group_by(site, month) %>%\n  summarise(mean_temp = round(mean(temp, na.rm = T)),\n            .groups = \"drop\") %>%\n  arrange(mean_temp) %>%\n  select(mean_temp) %>%\n  group_by(mean_temp) %>% \n  summarise(count = n(), \n            .groups = \"drop\")\n\nggplot(data = sst_NOAA_n, aes(x = 1:nrow(sst_NOAA_n), y = mean_temp)) +\n  geom_point(aes(size = count)) +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\nWe see now when we include the count (n) of the different mean temperatures that this distribution is not so even. There appears to be a hump around 20°C to 23°C. Of course, we’ve created dot plots here just to illustrate this point. In reality if one were interested in a distribution like this one would use a histogram, or better yet, a density polygon.\n\nsst_NOAA %>% \n  mutate(month = lubridate::month(t)) %>%\n  group_by(site, month) %>%\n  summarise(mean_temp = round(mean(temp, na.rm = T)), \n            .groups = \"drop\") %>% \n  ggplot(aes(x = mean_temp)) +\n  geom_density(aes(fill = site), alpha = 0.6) +\n  labs(x = \"Temperature (°C)\")\n\n\n\nSelect observations (rows) by number with slice()\nIf one wants to select only specific rows of a dataframe, rather than using some variable like we do for filter(), we use slice(). The function expects us to provide it with a series of integers as seen in the following code chunk. Try playing around with these values and see what happens\n\n# Slice a sequence of rows\nsst_NOAA %>% \n  slice(10010:10020)\n\n# Slice specific rows\nsst_NOAA %>%\n  slice(c(1,8,19,24,3,400))\n\n# Slice all rows except these\nsst_NOAA %>% \n  slice(-(c(1,8,4)))\n\n# Slice all rows except a sequence\nsst_NOAA %>% \n  slice(-(1:1000))\n\nIt is discouraged to use slice to remove or select specific rows of data as this does not discriminate against any possible future changes in ones data. Meaning that if at some point in the future new data are added to a dataset, re-running this code will likely no longer be selecting the correct rows. This is why filter() is a main function, and slice() is not. This auxiliary function can however still be quite useful when combined with arrange.\n\n# The top 5 variable sites as measured by SD\nsst_NOAA %>% \n  mutate(month = lubridate::month(t)) %>% \n  group_by(site, month) %>% \n  summarise(sd_temp = sd(temp, na.rm = T), \n            .groups = \"drop\") %>% \n  arrange(desc(sd_temp)) %>% \n  slice(1:5)\n\n\n\n\nWorking directories\nAt the beginning of this page we glossed over this topic by setting the working directory via RStudio’s project functionality. This concept is however critically important to understand so we must now cover it in more detail. The current working directory, where R will read and write files, is displayed by RStudio within the title region of the Console. There are a number of ways to change the current working directory:\n\nSelect ‘Session’/‘Set Working Directory’ and then choose from the four options for how to set your working directory depending on your preference\nFrom within the Files pane, navigate to the directory you want to set as the working directory and then select ‘More’/‘Set As Working Directory’ menu item (navigation within the Files pane alone will not change the working directory)\nUse setwd(), providing the name of your desired working directory as a character string - this is the reccomended option of the three\n\nIn the Files tab, use the directory structure to navigate to the R Workshop directory (this will differ from person to person). Then under ‘More’, select the small upside down (drill-down) triangle and select ‘Set As Working Directory’. This means that whenever you read or write a file it will always be working in that directory. This gives us the code for setting the directory (below is the code that I would enter in the Console on my computer):\n\nsetwd(\"~/R_Workshop\")\n\nIt will be different for you, but copy it into your script and make a note for future reference.\n\nWorking directories\nFor Windows users, if you copy from a file path the slashes will be the wrong way around and must be changed!\n\nYou can check that R got this right by typing into the Console:\n\ngetwd()\n\n\nOrganising R projects\nFor every R project, set up a separate directory that includes the scripts, data files and outputs.\n\nBefore moving on it is important that everyone is comfortable with this concept because tomorrow (Day 6) we will be learning how to source, download, and process data from the wild. If we don’t have a clear understanding of working directories, this process is going to be difficult. Please let the instructor know now if you would like to spend more time on this concept instead of moving on to the DIY data assignment. It is perfectly fine if you do.\n\n\nData in R\nThe base R program that we all have loaded on our computers already comes with heaps of example dataframes that we may use for practice. We don’t need to load our own data. Additionally, whenever we install a new package (and by now we’ve already installed dozens) it usually comes with several new dataframes. There are many ways to look at the data that we have available from our packages. Below we show two of the many options.\n\n# To create a list of ALL available data\n  # Not really recommended as the output is overwhelming\ndata(package = .packages(all.available = TRUE))\n\n# To look for datasets within a single known package\n  # type the name of the package followed by '::'\n  # This tells R you want to look in the specified package\n  # When the autocomplete bubble comes up you may scroll\n  # through it with the up and down arrows\n  # Look for objects that have a mini spreadsheet icon\n  # These are the datasets\n\n# Try typing the following code and see what happens...\ndatasets::\n\nWe have an amazing amount of data available to us. So the challenge is not to find a dataframe that works for us, but to just decide on one. My preferred method is to read the short descriptions of the dataframes and pick the one that sounds the funniest. But please use whatever method makes the most sense to you. Remember that in R there are generally two different forms of data: wide OR long, and ggplot2 works much better with long data. To look at a dataframe of interest we use the same method we would use to look up a help file for a function.\nOver the years I’ve installed so many packages on my computer that it is difficult to chose a dataframe. The package boot has some particularly interesting dataframes with a biological focus. Please install this now to access to these data. I have decided to load the urine dataframe here. Note that library(boot) will not work on your computer if you have not installed the package yet. With these data we will now make a scatterplot with two of the variables, while changing the colour of the dots with a third variable.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(boot)\n\n# Load data\nurine <- boot::urine\n\n# Look at help file for more info\n# ?urine\n\n# Create a quick scatterplot\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = cond))\n\nAnd now we have a scatterplot that is showing the relationship between the osmolarity and pH of urine, with the conductivity of those urine samples shown in shades of blue. What is important to note here is that the colour scale is continuous. How can we know this by looking at the figure? Let’s look at the same figure but use a discrete variable for colouring.\n\nggplot(data = urine, aes(x = osmo, y = ph)) +\n  geom_point(aes(colour = as.factor(r)))\n\nWhat is the first thing you notice about the difference in the colours? Why did we use as.factor() for the colour aesthetic for our points? What happens if we don’t use this? Try it now. This is a brief recap of what we learned on Day 3."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content",
    "section": "",
    "text": "During the workshop there will be a guest lecture on the creation of a Policy Brief. This will be followed by a two day hands-on session where attendees will break into small groups to develop a policy brief by sourcing data, cleaning it, analysing it, plotting/mapping it, and writing up a brief based on their findings.\nLastly, there are some Bonus materials provided here for further exploration by the interested researchers."
  },
  {
    "objectID": "bonus.html#functions-for-creating-ant-walks",
    "href": "bonus.html#functions-for-creating-ant-walks",
    "title": "Bonus",
    "section": "Functions for creating ant walks",
    "text": "Functions for creating ant walks\n\n# Calculate speed based on u and v vectors\nant.speed <- function(df){\n  df$x2 <- c(NA,df$x[2:nrow(df)] - df$x[1:(nrow(df)-1)])\n  df$y2 <- c(NA,df$y[2:nrow(df)] - df$y[1:(nrow(df)-1)])\n  speed_abs <- round(sqrt(df$x2^2 + df$y2^2),2)\n  speed_abs[is.na(speed_abs)] <- 0\n  return(speed_abs)\n}\n\n# Create a dataframe with desired number of ants and steps\nant.walk <- function(i,n){\n  # Create the random walks\n  walk_x <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  for(i in 2:i){\n  x <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  walk_x <- c(walk_x, x)\n  }\n  walk_y <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  for(i in 2:i){\n  y <- c(0,round(cumsum(rnorm(n = n-1, mean = 0, sd = 1)),2))\n  walk_y <- c(walk_y, y)\n  }\n  # Create the walking dataframe\n  walker <- data.frame(x = walk_x, y = walk_y, \n                       ant = as.factor(rep(1:i, each = n)), \n                       step =  rep(seq(1,n), i))\n  walker$speed <- ant.speed(walker)\n  walker$speed[walker$step == 1] <- 0\n  return(walker)\n}"
  },
  {
    "objectID": "bonus.html#generate-the-ants",
    "href": "bonus.html#generate-the-ants",
    "title": "Bonus",
    "section": "Generate the ants",
    "text": "Generate the ants\n\nants <- ant.walk(5, 100)"
  },
  {
    "objectID": "bonus.html#the-function-to-animate-the-walk-plot",
    "href": "bonus.html#the-function-to-animate-the-walk-plot",
    "title": "Bonus",
    "section": "The function to animate the walk plot",
    "text": "The function to animate the walk plot\n\nwalk.plot <- function(i){\n  # Map figure\n  walk_map <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = x, y = y)) +\n    geom_path(aes( group = ant), colour = \"gray60\") +\n    geom_point(data = ants[ants$step == i,], aes(colour = ant))\n  # Speed histogram\n  walk_hist <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = speed)) +\n    geom_histogram() +\n    labs(x = \"speed\")\n  # Speed line graph\n  walk_line <- ggplot(data = ants[ants$step %in% 1:i,], aes(x = step, y = speed)) +\n    geom_line(aes(colour = ant))\n  # Wack it together\n  grid.arrange(walk_map, walk_hist, walk_line, layout_matrix = cbind(c(1,1), c(1,1), c(2,3)))\n}\n\n\n## Create animation of ts plots\nanimate.walk.plot <- function() {\n  lapply(seq(1,100), function(i) {\n    walk.plot(i)\n  })\n}"
  },
  {
    "objectID": "bonus.html#render-the-gif",
    "href": "bonus.html#render-the-gif",
    "title": "Bonus",
    "section": "Render the GIF",
    "text": "Render the GIF\n\n# By default 'saveGIF()' outputs to the same folder \n# the script where the code is being run from is located\n# I have included commented out lines of code here that \n# may be changed to set thedestination for saving the output\n# setwd(\"~/Intro_R_Workshop/BONUS/\")\nsystem.time(saveGIF(animate.walk.plot(), interval = 0.2, \n                    ani.width = 800, movie.name = \"ant_walk.gif\")) ## ~60 seconds\n# setwd(\"~/Intro_R_Workshop/\")"
  },
  {
    "objectID": "bonus.html#testing-assumptions",
    "href": "bonus.html#testing-assumptions",
    "title": "Bonus",
    "section": "Testing assumptions",
    "text": "Testing assumptions\nTo test the normality of the distribution of a set of data we may use the shapiro.test() function. This produces a ‘w’ score as well as a p-value, but for now we are only interested in the later. Anything above p = 0.05 may considered to be normally distributed.\nTo test for similarity of variance we will run the var() function. As long as no group of data has ~4 times greater variance than any other group we are comparing it against it will pass this test.\nWith the help of the %>% we may test all of our assumptions in one pass.\n\n# First test the Laminaria data by region\n  # This passes our tests\nlam_norm_region <- laminaria %>%\n  group_by(region) %>% \n  summarise(norm_length = shapiro.test(total_length)[2],\n            var_length = var(total_length)) %>% \n  ungroup()\n\n# Then test by site\n  # This no longer passes our tests\nlam_norm_site <- laminaria %>%\n  group_by(site) %>% \n  summarise(norm_length = shapiro.test(total_length)[2],\n            var_length = var(total_length)) %>% \n  ungroup()\n\n# Lastly we test the SACTN data\n  # Which also fails\nSACTN_norm <- SACTN %>% \n  group_by(index) %>% \n  summarise(norm_temp = shapiro.test(temp)[2],\n            var_temp = var(temp, na.rm = T))"
  },
  {
    "objectID": "bonus.html#comparison-of-two-means",
    "href": "bonus.html#comparison-of-two-means",
    "title": "Bonus",
    "section": "Comparison of two means",
    "text": "Comparison of two means\nTo run a t-test we use t.test(). The argument this function wants is in the form of a formula. This requires to bits of information separated by a ~. On the left we provide the name of the column containing the variable we want to compare between two groups. On the right we put the column containing the grouping variable. The second argument we provide is data = x, where we tell R what the name of the dataframe is that contains the columns we have fed to the formula.\n\nt.test(total_length ~ region, data = laminaria)\n\nHappily the Laminaria data, when separated by region, pass our assumption tests. Had they not, we would need to use a Wilcox test instead of a t-test. Note that the arguments are written the exact same for both functions.\n\nwilcox.test(total_length ~ region, data = laminaria)"
  },
  {
    "objectID": "bonus.html#comparison-of-multiple-means",
    "href": "bonus.html#comparison-of-multiple-means",
    "title": "Bonus",
    "section": "Comparison of multiple means",
    "text": "Comparison of multiple means\nThe function we use to compare multiple means that pass our assumption tests (parametric data) are aov() for an ANOVA and for non-parametric data we use kruskal.test() for a Kruskal-Wallis test. To see the difference between the individual factor levels within our multiple means comparisons tests we use TukeyHSD() for parametric data and kruskalmc() for non-parametric data. Note that aov() does not by defalut output the information we are after so we wrap it inside of summary(). Note that the Laminaria and SACTN data violate our assumptions. We should therefore not perform paramteric tests on them. We do so below to highlight how these tests work should one have parametric data to use.\n\n# Look at the significance results for Laminaria total lengths\nsummary(aov(total_length ~ site, data = laminaria))\n\n# Look at the Tukey test results\nTukeyHSD(aov(total_length ~ site, data = laminaria))\n\n# Multi-level ANOVA\n  # Interaction between factorial levels\nsummary(aov(total_length ~ region * site, data = laminaria))\n\nTukeyHSD(aov(total_length ~ region * site, data = laminaria))\n\n# Single level non-parametric test\nkruskal.test(total_length ~ as.factor(site), data = laminaria)\n\n# Post-test\nkruskalmc(total_length ~ as.factor(site), data = laminaria)"
  },
  {
    "objectID": "bonus.html#correlation",
    "href": "bonus.html#correlation",
    "title": "Bonus",
    "section": "Correlation",
    "text": "Correlation\nTo check for the correlation between multiple values we may use cor(). This may be done in the pipe very quickly, but we have also provided below how to perfomr this test using the base R syntax.\n\n# Using the pipe\n  # This allows us to compare multple columns of our choosing easily\nlaminaria %>% \n  select(-(region:Ind)) %>% \n  cor(., use = \"complete.obs\")\n\n# Or base R syntax\n  # Here we must specify individual columns\ncor(laminaria$digits, laminaria$blade_length)"
  },
  {
    "objectID": "bonus.html#regression-analysis",
    "href": "bonus.html#regression-analysis",
    "title": "Bonus",
    "section": "Regression analysis",
    "text": "Regression analysis\nThe last analysis we will look at in this tut is regression analysis. This is performed by running a linear model, lm(), on two columns of data. We do so with the formula notation that we saw earlier but now the righ side of the ~ contains the dependant variable, and the left side the independent.\n\n# The summary\nsummary(lm(stipe_diameter ~ stipe_mass, data = laminaria))\n\n# Plot the R2 value\nggplot(data = laminaria, aes(x = stipe_mass, y = stipe_diameter)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  stat_poly_eq(formula = y ~ x, \n                aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")), \n                parse = TRUE)"
  },
  {
    "objectID": "bonus.html#mangle",
    "href": "bonus.html#mangle",
    "title": "Bonus",
    "section": "Mangle",
    "text": "Mangle\nAnd now begins the mangling.\n\n# Sites to extract\nsites <- c(\"Med\", \"NW_Atl\", \"WA\")\n\n# Create tidy base\nOISST_tidy <- sst_NOAA %>%\n  mutate(year = year(t)) %>%\n  filter(site %in% sites,\n         year %in% c(2008, 2009)) %>%\n  select(-year)\n\n# First mangle\n  # Normal tidy data\nOISST1 <- OISST_tidy\n\n# Second mangle\nOISST2 <- OISST_tidy %>%\n  pivot_wider(names_from = site, values_from = temp)\n\n# Third mangle\nOISST3 <- OISST_tidy %>%\n  mutate(t = as.character(t),\n         idx = 1:n()) %>% \n  pivot_longer(cols = c(site, t), names_to = \"type\", values_to = \"name\") %>% \n  dplyr::select(idx, type, name, temp)\n\n## Fourth two part mangle\n# A\nOISST4a <- OISST_tidy %>%\n  mutate(t = as.character(t)) %>%\n  unite(index, site, t, sep = \" \")\n\n# B\nOISST4b <- OISST_tidy %>%\n  mutate(t = as.character(t),\n         idx = 1:n()) %>%\n  separate(col = t, into = c(\"year\", \"month\", \"day\"), sep = \"-\") %>%\n  select(-temp)"
  },
  {
    "objectID": "bonus.html#save",
    "href": "bonus.html#save",
    "title": "Bonus",
    "section": "Save",
    "text": "Save\nHere we save all five of the newly mangled dataframes as one .RData object for ease of loading in the tutorial.\n\nsave(list = c(\"OISST1\", \"OISST2\", \"OISST3\", \"OISST4a\", \"OISST4b\"), file = \"course_material/data/OISST_mangled.RData\")"
  },
  {
    "objectID": "bonus.html#date-details",
    "href": "bonus.html#date-details",
    "title": "Bonus",
    "section": "Date details",
    "text": "Date details\nLook at strip time format for guidance\n\n?strptime\n\nCheck the local time zone\n\nSys.timezone(location = TRUE)"
  },
  {
    "objectID": "bonus.html#creating-daily-dates",
    "href": "bonus.html#creating-daily-dates",
    "title": "Bonus",
    "section": "Creating daily dates",
    "text": "Creating daily dates\nCreate date columns out of the mangled date data we have loaded.\n\n# Create good date column\nnew_dates <- sad_dates %>%\n  mutate(new_good = as.Date(good))\n\n# Correct bad date column\nnew_dates <- new_dates %>%\n  mutate(new_bad = as.Date(bad, format = \"%m/%d/%y\"))\n\n# Correct ugly date column\nnew_dates <- new_dates %>%\n  mutate(new_ugly = seq(as.Date(\"1998-01-13\"), as.Date(\"1998-01-21\"), by = \"day\"))"
  },
  {
    "objectID": "bonus.html#creating-hourly-dates",
    "href": "bonus.html#creating-hourly-dates",
    "title": "Bonus",
    "section": "Creating hourly dates",
    "text": "Creating hourly dates\nIf we want to create date values out of data that have hourly values (or smaller), we must create ‘POSIXct’ valus because ‘Date’ values may not have a finer temporal resolution than one day.\n\n# Correcting good time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_good_hours = as.POSIXct(good_hours, tz = \"Africa/Mbabane\"))\n\n\n# Correcting bad time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_bad_hours = as.POSIXct(bad_hours, format = \"%Y-%m-%d %I:%M:%S %p\", tz = \"Africa/Mbabane\"))\n\n\n# Correcting bad time stamps with hours\nnew_dates <- new_dates %>%\n  mutate(new_ugly_hours = seq(as.POSIXct(\"1998-01-13 09:00:00\", tz = \"Africa/Mbabane\"),\n                              as.POSIXct(\"1998-01-13 17:00:00\", tz = \"Africa/Mbabane\"), by = \"hour\"))\n\nBut shouldn’t there be a function that loads dates correctly?"
  },
  {
    "objectID": "bonus.html#importing-dates-in-one-step",
    "href": "bonus.html#importing-dates-in-one-step",
    "title": "Bonus",
    "section": "Importing dates in one step",
    "text": "Importing dates in one step\nWhy yes, yes there is. read_csv() is the way to go.\n\nsmart_dates <- read_csv(\"../data/sad_dates.csv\")\n\nBut why does it matter that we correct the values to dates? For starters, it affects the way our plots look/work. Let’s create some random numbers for plotting and see how these compare against our date values when we create figures.\n\n# Generate random number\nsmart_dates$numbers <- rnorm(9, 2, 10)\n\n# Scatterplot with correct dates\nggplot(smart_dates, aes(x = good, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n# Scatterplot with incorrect dates\nggplot(smart_dates, aes(x = bad, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n# OR\nggplot(smart_dates, aes(x = ugly, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\nIf the dates are formatted correctly it also allows us to do schnazy things with the data.\n\nsmart_dates$good[4]+32\nsmart_dates$good[9]-smart_dates$good[3]\nas.Date(smart_dates$good[9]:smart_dates$good[3])\nsmart_dates$good[9]-10247"
  },
  {
    "objectID": "bonus.html#gif-creation",
    "href": "bonus.html#gif-creation",
    "title": "Bonus",
    "section": "GIF creation",
    "text": "GIF creation\nOnce we have loaded our base image and the GIF we want to put on top of it we need to create a function to make these two different file types ‘kiss’. With the appropriately named magick package this is startlingly easy to do.\n\nframes <- lapply(anim_overlay, function(frame) {\n  image_composite(background, frame, offset = \"+300\")\n})"
  },
  {
    "objectID": "bonus.html#gif-animation",
    "href": "bonus.html#gif-animation",
    "title": "Bonus",
    "section": "GIF animation",
    "text": "GIF animation\nWith our function for creating the GIF sorted, it is now time to animate it!\n\nanimation <- image_animate(image_join(frames), fps = 10) # FPS = 10 is native speed"
  },
  {
    "objectID": "bonus.html#gif-save",
    "href": "bonus.html#gif-save",
    "title": "Bonus",
    "section": "GIF save",
    "text": "GIF save\nJip. Simple as that.\n\nimage_write(animation, \"../BONUS/carlton.gif\")"
  },
  {
    "objectID": "bonus.html#jsonlite",
    "href": "bonus.html#jsonlite",
    "title": "Bonus",
    "section": "jsonlite",
    "text": "jsonlite\nWith new R capabilities comes the requirement for at least one new package. So let’s go ahead and install that.\n\nlibrary(tidyverse)\n\n# Package for reading JSON data\n# install.packages(\"jsonlite\")\nlibrary(jsonlite)\n\n# Package for dealing with spatial data\n# install.packages(\"raster\")\nlibrary(raster)\n\n# Packages for changing dates\n# install.packages(\"lubridate\")\nlibrary(lubridate)\n# install.packages(\"zoo\")\nlibrary(zoo)\n\n# Packages for plotting\nlibrary(ggmap)\n\n# A script containing several custom functions\nsource(\"markdown/mapping_yourself_func.R\")\n\nTo download your Google location history please sign in to your Google account (if you aren’t already) and then click the following link: https://takeout.google.com/settings/takeout. Once you are at the download page please make sure you select only “location history” for download, otherwise you will be waiting a long time for the download to finish.\nThe format of the data you will download is .json. Don’t worry about this as we now have the jsonlite package to do the hard work for us. It may take your computer a couple of minutes to load your data into R. Some of your files may be quite large if Google has been tracking you more closely…\n\n# Note that this file is not in the Intro R Workshop folder\n# You will need to download your own data to follow along\n# I may provide you with my history if you have none\n# location_history <- fromJSON(\"data/LocationHistory.json\")\n# save(location_history, file = \"data/location_history.Rdata\")\nload(\"../data/location_history.RData\")"
  },
  {
    "objectID": "bonus.html#check-the-data",
    "href": "bonus.html#check-the-data",
    "title": "Bonus",
    "section": "Check the data",
    "text": "Check the data\nWith our Google location history data loaded into R we may now start to clean it up so we can create maps and perform analyses.\n\n# extract and clean the locations dataframe\nloc <- location.clean(location_history)\n\nNow that we’ve cleaned up the data, let’s see what we’re dealing with.\n\n# Number of times our position was recorded\nloc %>% \n  nrow()\n# The date Google started tracking us\nloc %>%\n  summarise(min(time))\n# The most recent date in Googles memory banks\nloc %>%\n  summarise(max(time))\n\nTo calculate the number of days, months and years of data Google has on us we will use the following code.\n\n# Count the number of records per day\npoints_p_day <- loc %>% \n  group_by(date) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"day\")\n\n# Count the number of records per month\npoints_p_month <- loc %>% \n  group_by(month_year) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"month\") %>% \n  rename(date = month_year)\n\n# Count the number of records per year\npoints_p_year <- loc %>% \n  group_by(year) %>%\n  summarise(count = n()) %>% \n  mutate(group = \"year\") %>% \n  rename(date = year)\n\n# Number of days/ months/ years recorded\nnrow(points_p_day)\nnrow(points_p_month)\nnrow(points_p_year)"
  },
  {
    "objectID": "bonus.html#where-in-the-world-are-you",
    "href": "bonus.html#where-in-the-world-are-you",
    "title": "Bonus",
    "section": "Where in the world are you?",
    "text": "Where in the world are you?\nIf this hasn’t been creepy enough, just wait, there’s more! Now we are going to create maps from the data collected on us. Due to the impressive quality of these data there are quite a few sophisticated things we may do with them. We will work through several examples together. The first will be a boxplot.\n\n# First create a dataframe for all of your points of data\n# The [, -1] is removing the 'date' column from each dataframe\npoints <- rbind(points_p_day[,-1], points_p_month[,-1], points_p_year[,-1])\n\n# Now for the figure\nggplot(points, aes(x = group, y = count)) + # The base of the mfigure\n  geom_boxplot(aes(colour = group), size = 1, outlier.colour = NA) + # The boxplot\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) + # Our data points\n  facet_grid(group ~ ., scales = \"free\") + # Facet by day/ month/ year\n    labs(x = \"\", y = \"Number of data points\") + # Change the labels\n  theme(legend.position = \"none\", # Remove the legend\n    strip.background = element_blank(), # Remove strip background\n    strip.text = element_blank()) # Remove strip text\n\nThis shows us how many data points Google tends to collect about us every day, month and year. Why did we plot each boxplot in it’s own panel?\nUp next we will look at the map of all of these points.\n\n# First we must download the  map of South Africa\n# south_africa <- get_map(location = 'GSouth Africa', zoom = 5)\nload(\"../data/south_africa.RData\")\n\n# Then we may plot our points on it\nggmap(south_africa) + \n  geom_point(data = loc, aes(x = lon, y = lat), \n             alpha = 0.5, colour = \"red\") + \n  labs(x = \"\", y = \"\")\n\nNow let’s focus on the Cape Town area specifically.\n\n# Download Cape Town map\n# cape_town <- get_map(location = 'Cape Town', zoom = 12)\nload(\"../data/cape_town.RData\")\n\n# Create the map\nggmap(cape_town) + \n  geom_point(data = loc, aes(x = lon, y = lat), \n             alpha = 0.5, colour = \"khaki\") +\n  labs(x = \"\", y = \"\")\n\nRemember earlier how I said these Google data were very high quality and we could do all sorts of analyses with them? One of the additional things Google tracks is our velocity. So we don’t even need to calculate it. We may just plot it as is.\n\n# Create a data frame with no NA values for velocity\nloc_2 <- loc %>% \n  na.omit(velocity)\n\nggmap(cape_town) + \n  geom_point(data = loc_2, \n             aes(x = lon, y = lat, colour = velocity), alpha = 0.3) + \n  scale_colour_gradient(low = \"blue\", high = \"red\", \n                        guide = guide_legend(title = \"Velocity\")) +\n  labs(x = \"\", y = \"\")\n\nIf the map above is too zoomed in to see your data try changing the level of the zoom argument."
  },
  {
    "objectID": "bonus.html#big-brother",
    "href": "bonus.html#big-brother",
    "title": "Bonus",
    "section": "Big Brother",
    "text": "Big Brother\nFor the end of this session we are going to perform two more analyses. The first will be to see how far Google knows that we travel when it is tracking us. And from that we will then understand how Google guesses what it thinks we are doing. Yes, Google’s data mining algorithms do think about what you do and record those assumptions. Another service provided by your friendly neighbourhood SkyNet.\n\n# Create a distance dataframe\ndistance_p_month <- distance.per.month(loc)\n\n# The distance in KM's Google has tracked you\ndistance_p_month %>% \n  summarise(sum(distance))\n\n# A bar plot of the distances tracked\nggplot(data = distance_p_month, \n       aes(x = month_year, y = distance,  fill = as.factor(month_year))) +\n  geom_bar(stat = \"identity\")  +\n  guides(fill = FALSE) +\n  labs(x = \"\", y = \"Distance (km)\")\n\nLastly, let’s take a peek at what it is Google thinks we are doing with ourselves. Because Google records activity probabilities for each tick of its watch, only the activity with highest likelihood at that time is chosen.\n\n# Create the activities dataframe\nactivities <- activities.df(location_history)\n\n# The figure\nggplot(data = activities, \n       aes(x = main_activity, group = main_activity, fill = main_activity)) +\n  geom_bar()  +\n  guides(fill = FALSE) +\n  labs( x = \"\", y = \"Count\")"
  },
  {
    "objectID": "bonus.html#morph-creation",
    "href": "bonus.html#morph-creation",
    "title": "Bonus",
    "section": "Morph creation",
    "text": "Morph creation\n\nmorph_frames <- image_morph(c(oldlogo, newlogo), frames = 100)"
  },
  {
    "objectID": "bonus.html#morph-animation",
    "href": "bonus.html#morph-animation",
    "title": "Bonus",
    "section": "Morph animation",
    "text": "Morph animation\n\nmorph_animate <- image_animate(frames, fps = 20)"
  },
  {
    "objectID": "bonus.html#morph-save",
    "href": "bonus.html#morph-save",
    "title": "Bonus",
    "section": "Morph save",
    "text": "Morph save\n\nimage_write(morph_animate, \"../BONUS/morph.gif\")"
  },
  {
    "objectID": "bonus.html#mds",
    "href": "bonus.html#mds",
    "title": "Bonus",
    "section": "MDS",
    "text": "MDS\nMDS, or multi-dimensional scaling, is high level clustering technique. MDS allows us to determine which of the abiotic variables in our dataset are having the most pronounced effects on the clustering of the dunes. Running an MDS on a data frame in R is simple as the vegan package will do all of the heavy lifting for us. First we will jump straight in and run an MDS, then we will take a step back and try changing the standardisation of the values and the distance matrix that we would normally need to first calculate. Please consult the help file (?metaMDS) for details on the function.\n\ndune_MDS_1 <- metaMDS(dune)\n\nOr we may be more specific in the way in which we prepare our data for the MDS. Look through the help files to see what other options exist.\n\n# Standardise data\ndune_stand <- decostand(dune, method = \"total\")\n\n# Create Bray-Curtis dissimilarity matrix\ndune_dist <- vegdist(dune_stand, method = \"bray\")\n\n# Create distance matrix\ndune_MDS_2 <- metaMDS(dune_dist)"
  },
  {
    "objectID": "bonus.html#stress",
    "href": "bonus.html#stress",
    "title": "Bonus",
    "section": "Stress",
    "text": "Stress\nNo, not that stress. We are talking about the stress of the MDS model now. This is an important value to check. If the stress is high (>0.3) the MDS model is doing a poor job of modeling the dissimilarities in the data. If it is low (<0.1) the model is doing a very good job of displaying the relationships within the data. To check the stress of our results we use the following line of code.\n\n# Default MDS settings\ndune_MDS_1$stress\n\n# Determined settings\ndune_MDS_2$stress\n\nWhat is the stress of this model? Is that an acceptable level?"
  },
  {
    "objectID": "bonus.html#basic-biplot",
    "href": "bonus.html#basic-biplot",
    "title": "Bonus",
    "section": "Basic biplot",
    "text": "Basic biplot\nWith the MDS calculated, and the stress tested, it’s time to visualise the first round of results.\n\n# Convert for ggplot\ndune_MDS_points <- data.frame(site = 1:nrow(dune)) %>%\n  mutate(x = as.numeric(dune_MDS_2$points[ ,1]),\n         y = as.numeric(dune_MDS_2$points[ ,2]))\n\n# Visualise with ggplot\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  labs(x = \"NMDS1\", y = \"NMDS2\")"
  },
  {
    "objectID": "bonus.html#fitting-environmental-variables",
    "href": "bonus.html#fitting-environmental-variables",
    "title": "Bonus",
    "section": "Fitting environmental variables",
    "text": "Fitting environmental variables\nAs with all of the other ordination analyses we have performed in R thus far, fitting environmental variables may also be done with one easy step. We do this by providing the envfit() function with a formula, the same as we do for linear models. The dependent variable (to the left of the ~) will be the results of the MDS on the species assemblage data, and the independent variables (to the right of the ~) are the columns from our environmental variables data frame.\n\ndune_envfit <- envfit(dune_MDS_2 ~ Moisture + Use, data = dune.env)\ndune_envfit\n\nIn the printout above we see the results for the R^2 (here r2) and p-values for the fit of each abiotic variable to the species assemblage data. Which relationships are significant? Which variable(s) appears to best explain the variance in the species assemblages? Which of the axes of the MDS have the strongest relationship with which variable?\nTo plot the results of our fitted abiotic variables on top of our species MDS we need to quickly prep it to play nice with ggplot2 and then we need only append a couple of lines onto the chunk we wrote to display our MDS results.\n\n# Extract the envfit vector values\ndune_envfit_df <- data.frame(dune_envfit$factors$centroids) %>%\n  mutate(factors = row.names(.)) %>%\n  rename(x = NMDS1, y = NMDS2)\n\n# Visualise environmental fits\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  labs(x = \"NMDS1\", y = \"NMDS2\")"
  },
  {
    "objectID": "bonus.html#adding-clusters",
    "href": "bonus.html#adding-clusters",
    "title": "Bonus",
    "section": "Adding clusters",
    "text": "Adding clusters\nIn order to add clustering we must first create groupings for our data. In this instance we will be calculating our groups using hierarchical cluster analysis.\n\n# Create dendrogram\n  # Note that this must be run on a distance matrix\ndune_clust <- hclust(dune_dist, \"ward.D\")\n\n# Extract clusters\n  # In this case we have decided on four clusters\ndune_grp <- cutree(dune_clust, 4)\n\n# Extract groups for plotting\ndune_MDS_points <- dune_MDS_points %>% \n  mutate(grp_id = as.factor(dune_grp))\n\nWith the clusters calculated we may now plot ellipses on our biplot. We will first do this with the built-in functionality of ggplot2, which unfortunately isn’t great.\n\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  # The ellipses\n  stat_ellipse(aes(colour = grp_id), type = \"t\") + \n  #\n  labs(x = \"NMDS1\", y = \"NMDS2\", colour = \"Cluster\")\n\nIf we have very large datasets the ellipses will come more in line with what we want. With small datasets not so much. This is because the ellipses are actually calculating the area under which a certain confidence interval is maintained that the points in that group may be found. If we would rather use polygons to fit directly onto the area of our clusters we do so by replacing the ellipses with the following line of code.\n\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  geom_point(size = 8, shape = 21, fill = \"black\", colour = \"red\") +\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, arrow = arrow(length = unit(0.25, \"cm\")),\n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\", \n            aes(x = x, y = y, label = factors)) +\n  # The custom made polygons\n  stat_chull(geom = \"polygon\", aes(fill = grp_id), alpha = 0.4) +\n  #\n  labs(x = \"NMDS1\", y = \"NMDS2\")\n\nI’m not super excited about that result either. A third option is to simply change the colour of the points to reflect their grouping.\n\nggplot(data = dune_MDS_points, aes(x = x, y = y)) +\n  # Changing point aesthetics\n  geom_point(size = 8, aes(colour = grp_id)) +\n  #\n  geom_text(aes(label = site), colour = \"white\") +\n  geom_segment(data = dune_envfit_df, \n               aes(x = 0, y = 0, xend = x, yend = y)) +\n  geom_text(data = dune_envfit_df, colour = \"red\",\n            aes(label = factors)) +\n  labs(x = \"NMDS1\", y = \"NMDS2\", colour = \"Cluster\")\n\nI think this is actually the cleanest way to visualise the data."
  },
  {
    "objectID": "bonus.html#diversity",
    "href": "bonus.html#diversity",
    "title": "Bonus",
    "section": "Diversity",
    "text": "Diversity\nIf we are interested in calculating a Shannon-Wiener index on the species diversity found within the dunes we need only one function.\n\ndiversity(dune)"
  },
  {
    "objectID": "bonus.html#anosim",
    "href": "bonus.html#anosim",
    "title": "Bonus",
    "section": "ANOSIM",
    "text": "ANOSIM\nOne final thing. It is also necessary to know if any differences exist between the clusters we have determined for our data. To do this we use the anosim() function from the vegan package.\n\nanosim(dune_dist, dune_grp)"
  },
  {
    "objectID": "bonus.html#quick-examples",
    "href": "bonus.html#quick-examples",
    "title": "Bonus",
    "section": "Quick examples",
    "text": "Quick examples\nBelow is just a quick overview of the many common things one will need to know to put an RMarkdown document together.\n\nText\nThis is text in italics.\nAnd this text is in bold.\nThis text is in code font.\nYou can embed an R code chunk like this and show it and the data it produces:\n\nlibrary(ggmap)\n# load(\"../data/cape_point_sites.Rdata\")\n# cape_point <- get_map(location = c(lon = 18.36519, lat = -34.2352581),\n#                         zoom = 11, maptype = 'roadmap')\n# load(\"../data/cape_point.Rdata\")\n# str(cape_point)\n\n  # *   site name --- `r cape_point_sites[1,1]`\n  # *   longitude --- `r cape_point_sites[1,2]`\n  # *   latitude --- `r cape_point_sites[1,3]`\n\nYou can also embed R output directly into sentences as in this example:\nSome site details are in this list:\n\n\nTables\nThere are many ways to produce tables in R Markdown. A short search will provide many alternatives. The xtable package is another excellent choice as this provides even more options for how your table output will appear. Here we provide one example:\nknitr::kable(\n  head(mtcars[, 1:8], 10), booktabs = TRUE,\n  caption = 'A table of the first 10 rows of the mtcars data.'\n)\nTry looking up the help file for ?kable() to learn more about what may be done with this function.\n\n\nImages\nImages stored on your computer, such as \\(\\ref{fig:accurate}\\), can be embedded in your document and even cross referenced.\n\n\n\nNotice that in order to display images in this way we need to make sure R knows we are using the knitr package function include_graphics(), which allow one to display images of all sorts of file types without any fuss.\nYou can also embed any plots produced by R, for example:\n\n\n\nNotice above how the first line specifying the start of the R code includes some specifications with regards to the size of the figure, its caption, etc. Note too that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. The options results = TRUE, message = TRUE and warning = TRUE have similar functions. Info on the other code chunk options can be found at the R Markdown website or in the Cheatsheets and other documentation accessible via the RStudio ‘Help’ in the menu bar.\n\nHTML hyperlinks\nSee how the code above also demonstrates how to embed links to external websites.\n\n\n\nReferences\nWe can also have some references… This document was made using the R software [@R2017] and various add-on packages [@vegan2017]. The vegan package was produced by @vegan2017 some time ago."
  },
  {
    "objectID": "bonus.html#creating-a-document",
    "href": "bonus.html#creating-a-document",
    "title": "Bonus",
    "section": "Creating a document",
    "text": "Creating a document\nEven though we may immediately begin authoring documents with RStudio, we are limited to .html and .doc file types. If we want to author .pdf files, such as the one we are reading now, we must install ‘LaTeX’ on our computers. This installation process is beyond the scope of this course but there are many resources available online to aid one in the process and the software is, of course, free.\nWhether or not you have ‘LaTeX’ installed, when you click the Knit button (with the option to create multiple document kinds) a document will be generated that includes both content as well as the output of any embedded R code chunks (portions of R code surrounded by code that denotes the R commands) within the document. R code chunks can be used to render R output into documents or to simply display code for illustration, as outlined above.\nThis is a terribly basic demonstration, but since beautiful documentation already exists I suggest you go and find the necessary examples on the R Markdown website indicated above for a more in-depth account of how to use it."
  },
  {
    "objectID": "plots.html",
    "href": "plots.html",
    "title": "Plotting in R",
    "section": "",
    "text": "Though it may have started as statistical software, R has moved far beyond it’s mundane origins. The language is now capable of a wide range of applications, some of which you have already seen, and some others you will see over the rest of this workshop. For Day 3 we are going to go more in-depth on the concept of data visualisation via ggplot2. One should also note that there is a staggering amount of support for this package in the form of extension packages covering a range of visualisations and applications. A full gallery of the possible visualisations has been assembled and is worth a look."
  },
  {
    "objectID": "plots.html#slides-and-application-exercises",
    "href": "plots.html#slides-and-application-exercises",
    "title": "Plotting in R",
    "section": "Slides and application exercises",
    "text": "Slides and application exercises\n\nPlotting 1: Basics\n\nSlides\n\n\nSource\n\n\n\nTo aes() or not to aes()…\n\nSource\n\n\n\nPlotting 2: Facets\n\nSlides\n\n\nSource\n\n\n\nOh yeah. It’s all coming together.\n\nSource\n\n\n\nPlotting 3: Colours and stats\n\nSlides\n\n\nSource\n\n\n\nIt’s a colourful life.\n\nSource"
  },
  {
    "objectID": "plots.html#diy-figures",
    "href": "plots.html#diy-figures",
    "title": "Plotting in R",
    "section": "DIY figures",
    "text": "DIY figures\nToday we learned the basics of ggplot2, how to facet, how to brew colours, and how to plot stats. That’s a lot of stuff to remember! Which is why we will now spend the rest of Day 3 putting our new found skills to use. Please group up as you see fit to produce your very own ggplot2 figures. We’ve not yet learned how to manipulate/tidy up our data so it may be challenging to grab any dataset and make it work. To that end we recommend using either the sst_NOAA dataset we saw on Day 1, or penguins. You are of course free to use whatever dataset(s) you would like, including your own. The goal by the end of today is to have created at least two figures (first prize for four figures) and join them together via faceting. We will be walking the room to help with any issues that may arise."
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "Mapping in R",
    "section": "",
    "text": "Yesterday we learned how to create ggplot2 figures, change their aesthetics, labels, colour palettes, and facet/arrange them. Now we are going to look at how to create maps.\nMost of the work that we perform as environmental/biological scientists involves going out to a location and sampling information there. Sometimes only once, and sometimes over a period of time. All of these different sampling methods lend themselves to different types of figures. One of those, collection of data at different points, is best shown with maps. As we will see over the course of Day 3, creating maps in ggplot2 is very straight forward and is extensively supported. For that reason we are going to have plenty of time to also learn how to do some more advanced things."
  },
  {
    "objectID": "maps.html#slides-and-application-exercises",
    "href": "maps.html#slides-and-application-exercises",
    "title": "Mapping in R",
    "section": "Slides and application exercises",
    "text": "Slides and application exercises\n\nMapping 1: Basics\n\nSlides\n\n\nSource\n\n\n\nThe current landscape.\n\nSource\n\n\n\nMapping 2: Style\n\nSlides\n\n\nSource\n\n\n\nHere there be dragons.\n\nSource \n\n\n\nMapping 3: Arctic\n\nSlides\n\n\nSource\n\n\n\nA polar-ising subject.\n\nSource"
  },
  {
    "objectID": "maps.html#diy-maps",
    "href": "maps.html#diy-maps",
    "title": "Mapping in R",
    "section": "DIY maps",
    "text": "DIY maps\nNow that we have learned how to make conventional maps, as well as polar projections, it is time for us to branch out once again at let our creative juices flow. Please group up as you see fit and create your own beautiful map of wherever you like. Bonus points for faceting in additional figures showing supplementary information. Feel free to use either conventional maps or the polar alternative. Same as yesterday, we will be walking the room to help with any issues that may arise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This website contains the information and course content for the 2022 FACE-IT School ‘A Roundtrip From Data Handling to Data Presentation’. The source code for which is available on GitHub.\nThe aim of this workshop is to guide one through the use of R via RStudio for the analysis of environmental and biological data, with a focus on the Arctic. This workshop is ideal for eco/biologists new to R or who have limited experience, but should be useful to more advanced useRs as well. This workshop is intentionally selective, rather than providing a general overview of R. It will not go into hardcore statistics, but rather focuses on what will be useful to eco/biologists who have an interest in statistics, and use R frequently. The emphasis is thus on the steps required to source, analyse, and visualise data in R, rather than focusing on the statistical theory."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome",
    "section": "Overview",
    "text": "Overview\nThe core of the teaching content is laid out over a five-day period. This is preceded by two half days (Day 1-2) to ensure that everyone has R + RStudio and Git + Github setup correctly on their computers. Everything learned over the five days of instruction is then put to use by a two day group project to create a policy brief based on real-world data. The workshop is designed to begin simply and slowly to impart the basics of using R. It then gathers pace, so that by the end we are doing intermediate level analyses and visualisations on data we have sourced ourselves. On the first full day of instruction (Day 3), we consider the philosophy of data visualisation (i.e the grammar of graphics). Day 4 focuses on creating maps, concluding with the Arctic. Day 5 will see us coming to terms with the necessities of tidy data. On Day 6 we venture out into the wild to learn how to tame data for ourselves. And Day 7 ties it all back together. The workshop is case-study driven, using data and examples primarily from a background in the marine sciences and real life situations. There is no homework, but there are in class assignments at the end of each day.\nDon’t worry if you feel overwhelmed and do not follow everything at every point during the Workshop; that is totally natural with learning a new and powerful program. Remember that you have the notes and material to go through the exercises later at your own pace; an instructor will also be walking the room during sessions and breaks so that they can answer questions one on one. It is the hope that this Workshop gives you the confidence to start incorporating R into your daily workflow, and if you are already a useR, it is the hope that it will expose you to some new ways of doing things.\nFinally, bear in mind that there are many, many, many ways to accomplish the same task in R. The methods presented in this workshop are just one, but you will learn as you gain more experience with programming that there are many ways to get the right answer or to accomplish the same task."
  },
  {
    "objectID": "index.html#venue-date-and-time",
    "href": "index.html#venue-date-and-time",
    "title": "Welcome",
    "section": "Venue, date and time",
    "text": "Venue, date and time\nThis workshop will take place from 18 November – 28 November 2022, 9:00–17:00, at the Wadden Sea Station (AWI) on Sylt, Germany. Each day will have two 15 minute tea breaks and one hour for lunch. The 18th and 28th are set aside for travel, with no instruction planned on these dates. Note also that the first Saturday and Sunday are half days, devoted to managing administrative non-coding work (e.g. setting up R, RStudio, Git, and GitHub)."
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "Welcome",
    "section": "Course outline",
    "text": "Course outline\n\nDay 0 2022-11-18 – Arrival\n\nArrive in Sylt before 16:00 to get checked in at research station\nSocial dinner before start of workshop\n\n\n\nDay 1 2022-11-19 – R + RStudio\n\nInteractive Session: Preliminaries\nInteractive Session: R and RStudio functioning on all machines\n– break –\nInteractive Session: Introduction to R and RStudio\nExercise: It which shall not be named\n– lunch/end –\n\n\n\nDay 2 2022-11-20 – Git + GitHub\n\nInteractive Session: Git and Github functioning on all machines\n– break –\nExercise: Collaboration via GitHub\n– lunch/end –\n\n\n\nDay 3 2022-11-21 – Plots\n\nPresentation: The basics of ggplot2\nExercise: Basics\n– break –\nPresentation: Faceting figures in ggplot2\nExercise: Facets\n– lunch –\nPresentation: Brewing colours in ggplot2\nExercise: Colours\n– break –\nInteractive Session: R workflow - I\nAssignment: DIY figures\n– end –\n\n\n\nDay 4 2022-11-22 – Maps\n\nPresentation: Mapping with ggplot2\nExercise: Basic maps\n– break –\nPresentation: Mapping with style\nExercise: Fancy maps\n– lunch –\nPresentation: Mapping the Arctic\nExercise: Polar projections\n– break –\nInteractive Session: R workflow - II\nAssignment: DIY maps\n– end –\n\n\n\nDay 5 2022-11-23 – Tidy Data\n\nPresentation: Wrangling data\nExercise: Tidy data\n– break –\nPresentation: Taming data\nExercise: Tidier data\n– lunch –\nPresentation: Domesticating data\nExercise: Tidiest data\n– break –\nInteractive Session: R workflow - III\nAssignment: DIY tidy data\n– end –\n\n\n\nDay 6 2022-11-24 – Wild Data\n\nPresentation: Roaming data\nExercise: Wild and free\n– break –\nPresentation: Local data\nExercise: Full analyses\n– lunch –\nPresentation: Guest lecture on policy briefs\n– break –\nAssignment: Choose a policy brief topic and source wild data\n– end –\n\n\n\nDay 7 2022-11-25 – Recap\n\nPresentation: Recap\nInteractive Session: Q & A for policy brief choices\n– break –\nInteractive Session: Open Floor\n– lunch –\nInteractive Session: More Open Floor\n– end –\n\n\n\nDay 8 2022-11-26 – Policy brief\n\nBegin working on policy briefs\n\n\n\nDay 9 2022-11-27 – Policy brief\n\nFinish policy briefs\n\n\n\nDay 0 2022-11-28 – Departure\n\nCheck out of accommodation by 08:00"
  },
  {
    "objectID": "index.html#acknowledements",
    "href": "index.html#acknowledements",
    "title": "Welcome",
    "section": "Acknowledements",
    "text": "Acknowledements\n\n\n\n\n\nThis workshop was developed as a contribution of WP1 to the FACE-IT project. FACE-IT has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 869154.\nNote that much of the content in this workshop was developed from the BCBC honours class taught at the University of the Western Cape in South Africa."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Over the past four days we have covered quite a bit of ground. By now it is our hope that after having participated in this workshop you will feel confident enough using R to branch out on your own and begin applying what you have learned to your own research.\nAbove all, remember the tidy principles you have leaned here and endeavour to apply them to all facets of your work. The more uniformly tidy your work becomes, the more compounding benefits you will begin to notice."
  },
  {
    "objectID": "summary.html#the-future",
    "href": "summary.html#the-future",
    "title": "Summary",
    "section": "The future",
    "text": "The future\nThe content we have covered in this workshop is only the beginning. We have looked down upon the tidyverse, it’s multitudinous spiralling arms stretching out away from us in all directions. The next step is to begin to investigate the specific branches of the R tree of knowledge that interest us most. Or are most relevant to our work. The following list contains some further suggestions for workshops that are available:\n\nR for biologists\nR for environmental science\nR for oceanographers\nAdvanced visualisations\nMultivariate analysis\nSpecies distribution modelling\nReproducible research\nBasic stats\n\nFor further information or inquiries about additional training please contact Robert Schlegel: robwschlegel@gmail.com ."
  },
  {
    "objectID": "summary.html#today",
    "href": "summary.html#today",
    "title": "Summary",
    "section": "Today",
    "text": "Today\nFor the rest of today we will now open the floor to questions and suggestions that we may work through as a group."
  },
  {
    "objectID": "summary.html#useful-information",
    "href": "summary.html#useful-information",
    "title": "Summary",
    "section": "Useful information",
    "text": "Useful information\n\nOperators\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\nFunctions\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\)."
  },
  {
    "objectID": "summary.html#dataframes",
    "href": "summary.html#dataframes",
    "title": "Summary",
    "section": "Dataframes",
    "text": "Dataframes\nThe ‘workhorse’ data-containing structures you will use extensively in R are called dataframes. In fact, almost all of the work you do in R will be done directly with dataframes or will involve converting data into a dataframe. A dataframe is used for storing data as tables, with a table defined by a collection of vectors of similar or dissimilar data types but all of the same length. Don’t worry if any of those terms are unknown or daunting. We will cover them in detail just now. But first we need to see what a dataframe looks like in order to provide context for all of the parts they consist of. After we have covered all of the terms used for data in R we will learn some methods of creating our own dataframes.\nTo load a dataframe into R is quite simple when the data are already in the ‘.Rdata’ format. Let’s load a small dataframe that was prepared for this class and see. The file extension ‘.Rdata’ does not mean necessarily that the data are in a dataframe (table) format. This file extensions is actually a form of data compression unique to R and could hold anything from a single letter to the results of a complex species distribution model. For the following line of code to work we must make sure we are in the ‘Intro_R_Workshop’ project.\n\nload(\"data/intro_data.Rdata\")\n\nUpon loading the data frame we see in the Environment tab that there is a little blue circle next to our object. If we click on that we see a summary of each column. First it says what the data type for that column is and then shows the first several values therein.\nIf you click on the ‘intro_data’ word in your Environment tab it will open it in your Source Editor and allow you to click on the columns to organise them by ascending or descending order. Note that this does not change the dataframe, it is only a visual aid."
  },
  {
    "objectID": "summary.html#basic-data-types",
    "href": "summary.html#basic-data-types",
    "title": "Summary",
    "section": "Basic data types",
    "text": "Basic data types\nThere are several basic R data types that you frequently encounter in daily work. These include but are not limited to numeric, integer, logical, character, factor and date classes. All of these data types are present in our ‘intro_data’ dataframe for us to see practical examples. We will create our own examples as we go along.\n\nNumeric\nNumeric data with decimal values are called numeric in R. It is the default computational data type. If we look at our data frame we see that the following columns are numeric: lon, lat, NA.perc, mean, min and max. What sort of data are these?\nLet’s create our own numeric object by assigning a decimal value to a variable x as follows, x will be of numeric type:\n\nx <- 1.2 # assign 1.2 to x\nx # print the value of x\nclass(x) # what is the class of x?\n\nFurthermore, even if we assign a number to a variable k that doesn’t have a decimal place, it is still being saved as a numeric value:\n\nk <- 1\nk\nclass(k)\n\nIf we want to really be certain that k is or is not an integer we use is.integer():\n\nis.integer(k) # is k an integer?\n\n\n\nInteger\nAn integer in R is a numeric value that does not have a decimal place. It may only be a round whole number. Integers are often used for count data and when converting qualitative data to numbers for data analysis. In our dataframe we may see that we have two integer columns: depth and length. Why are these integers?\nIn order to create your own integer variable(s) in R, we use the as.integer(). We can be assured that y is indeed an integer by checking with is.integer():\n\ny <- as.integer(13)\ny\nclass(y)\nis.integer(y) # is it an integer?\n\nIf we really have to, we can coerce a numeric value into an integer with the same as.integer() function:\n\nz <- as.integer(pi)\nz\nclass(z)\nis.integer(z) # is it an integer?\n\n\n\nLogical\nThere are several logic values in R. We are mostly going to be concerned with the two main values we will be encountering: TRUE and FALSE. Note that all letters must be upper case. In our dataframe we see that only the ‘thermo’ column is logical. This column tells us whether or not the data were collected with a thermometer or not.\nLogical values (TRUE or FALSE) are often created via comparison between variables:\n\nx <- 1; y <- 2 # sample values\nz <- x > y\nz\nclass(z)\n\nIn order to perform logical operations we mostly use & (and), | (or), and ! (negation):\n\nu <- TRUE; v <- FALSE; w <- TRUE; x <- FALSE\nu & v\nu & w\nv & x\nu | v\n!u\n\nAlthough these logical operators can be immensely useful in more advanced R programming, we will not go into too much detail in this introductory course. For more information on the logical operators, see the R help material:\n\nhelp(\"&\")\n\nOne final thing to note about logic in R is that it can be useful to perform arithmetic on logical values. TRUE has the value 1, while FALSE has value 0:\n\nas.integer(TRUE) # the numeric value of TRUE\nas.integer(FALSE) # the numeric value of FALSE\nsum(as.integer(intro_data$thermo))\n\nWhat is this telling us?\n\n\nCharacter\nIn our dataframe we see that only the ‘src’ column has the character values. This column is showing us which government body etc. collected the data in that row. At the use of a very familiar word, character, one may think this data type must be the most straightforward. This is not necessarily so as character values are used to represent string values in R. Because computers do not understand text the same way we do, they tend to handle this information differently. This allows us to do some pretty wild stuff with character values, but we won’t be getting into that in this course as it quickly becomes very technical and generally speaking isn’t very useful in a daily application.\nIf however we wanted to convert an object to a character value we would do so with as.character():\n\nd <- as.character(pi)\nclass(d)\n\nThis can be useful if you have data that you want to be characters, but for one reason or another R has decided to make it a different data type.\nIf you want to join two character objects they can be concatenated with the paste() function:\n\na <- \"fluffy\"; b <- \"bunny\"\npaste(a, b)\npaste(a, b, sep = \"-\")\n\nMore functions for string manipulation can be found in the R documentation — type help(\"sub\") at the command prompt. You may also wish to install Hadley Wickham’s nifty stringr package for more cool ways to work with character strings.\n\n\nFactor\nFactor values are somewhat difficult to explain and often even more difficult to understand. Factor values appear the same as character values when we look at them in a spreadsheet. But they are not the same. This will lead to much wailing and gnashing of teeth. So why then do factors exist and why would we use them? Factors allow us to numerically order names non-alphabetically, for example. This then allows one to order a list of research sites in geographical order.\nWe will see many examples of factors during this course but for now look at the ‘site’ column in our dataframe. If we click on this column a couple of times we see that it reorders all the data based on ascending or descending order of the sites. But that order is not alphabetical, it is based on the levels within the factor column. Each factor value in a column is assigned a level integer value (e.g. 1, 2, 3, 4, etc.). If multiple values in a factor column are the same, they receive the same level value as well.\nIf we want to see what the levels within a factor column are we use levels():\n\nlevels(intro_data$site)\n\nWe will discuss in the next session what that $ means. But for now, are you able to see what the pattern is in the levels of the site listing?\nIf we want to create our own factors we will use as.factor():\n\nf <- as.factor(letters[1:5])\nlevels(f)\n\nAnd if we want to change the order of our factor levels we use factor():\n\nf <- factor(f, levels = c(\"b\", \"a\", \"c\", \"e\", \"d\"))\nlevels(f)\n\nAnother reason for using factors to re-order our data, as we shall see tomorrow, is that this allows us to control the order in which values are plotted.\n\n\nDates"
  },
  {
    "objectID": "summary.html#vectors",
    "href": "summary.html#vectors",
    "title": "Summary",
    "section": "Vectors",
    "text": "Vectors\nA vector, by definition, is a one-dimensional sequence of data elements of the same basic type (class). Members in a vector are officially called components. Basically, a vector is a column. Indeed, a dataframe is nothing more than a collection of vectors stuck together. If we wanted to create a vector from our dataframe we would do this:\n\nlonely_vector <- intro_data$NA.perc\n\nNotice that we may not click on the object lonely_vector in our Environment tab. This is because it is no longer two-dimensional. If we want to visualise the data we need to enter it into the console or run it from our script:\n\nlonely_vector\n\nLet’s create some vectors of our own:\n\nprimes1 <- c(3, 5, 7)\nprimes1\nclass(primes1)\n\np1 <- pi\np2 <- 5\np3 <- 7\n\nprimes2 <- c(p1, p2, p3)\nprimes2\nclass(primes2)\nis.numeric(primes2)\nis.integer(primes2) # integers coerced into floating point numbers\n\nWe can also have vectors of logical values or character strings, and we can use the function length() to see how many components each has:\n\ntf <- c(TRUE, FALSE, TRUE, FALSE, FALSE)\ntf\nlength(tf)\ncs <- c(\"Mary\", \"has\", \"a\", \"silly\", \"lamb\")\ncs\nlength(cs)\n\nOf course one would seldom enter data into R using the c() (combine) function, but it is useful for short calculations. More often than not one would import data from Excel (urgh!) or something more reputable. The kinds of data one can read into R are remarkable. We will get to that later on.\nWe can also combine vectors in many ways, and the simplest way is the append one after the other:\n\nprimes12 <- c(primes1, primes2)\nprimes12\n\nnonSense <- c(primes12, cs)\nnonSense\nclass(nonSense)\n\nIn the code fragment above, notice how the numeric values are being coerced into character strings when the two vectors of dissimilar class are combined. This is necessary so as to maintain the same primitive data type for members in the same vector."
  },
  {
    "objectID": "summary.html#vector-indices",
    "href": "summary.html#vector-indices",
    "title": "Summary",
    "section": "Vector indices",
    "text": "Vector indices\nWhat if we want to extract one or a few components from the vector? Easy… We retrieve values in a vector by declaring an index inside a single square bracket [] operator. For example, the following shows how to retrieve a vector component. Since the vector index is 1-based (i.e. the first component in a vector is numbered 1), we use the index position 7 for retrieving the seventh member:\n\nnonSense[7] # find the seventh component in the vector\n# or combine them in interesting ways...\npaste(nonSense[7], nonSense[8], nonSense[4], nonSense[10], \"bunnies\", sep = \" \")\n\nIf the index given is negative, it will remove the value whose position has the same absolute value as the negative index. For example, the following creates a vector slice with the third member removed. However, if an index is out-of-range, a missing value will be reported via the symbol NA:\n\na <- c(2, 6, 3, 8, 13)\na\na[-3]\na[10]"
  },
  {
    "objectID": "summary.html#vector-creation",
    "href": "summary.html#vector-creation",
    "title": "Summary",
    "section": "Vector creation",
    "text": "Vector creation\nR has many funky ways of creating vectors. This process is important to understand because we will need to build on it to create our own dataframes. Here are some examples of vector creation:\n\nseq(1:10) # assign them to a variable if you want to...\nseq(from = 0, to = 100, by = 10)\nseq(0, 100, len = 10) # one may omit from and to\nseq(1, 9, by = pi)\nrep(13, times = 13)\nrep(seq(1:5), times = 6)\na <- rnorm(20, mean = 13, sd = 0.13) # random numbers with known mean and sd\nrep(a, 5) # one may omit the times argument\nrep(c(\"A\", \"B\", \"C\"), 3)\nrep(c(\"A\", \"B\", \"C\"), each = 3)\nx <- c(\"01-31-1960\", \"02-13-1960\", \"06-23-1977\", \"01-01-2013\")\nclass(x)\nz <- as.Date(x, \"%m-%d-%Y\")\nclass(z) # introducing the date class\nseq(as.Date(\"2013-12-30\"), as.Date(\"2014-01-04\"), by = \"days\")\nseq(as.Date(\"2013-12-01\"), as.Date(\"2016-01-31\"), by = \"months\")\nseq(as.Date(\"2000/1/1\"), by = \"month\", length.out = 12)\n# and many more..."
  },
  {
    "objectID": "summary.html#vector-arithmetic",
    "href": "summary.html#vector-arithmetic",
    "title": "Summary",
    "section": "Vector arithmetic",
    "text": "Vector arithmetic\nArithmetic operations of vectors are performed component-by-component, i.e., componentwise. For example, suppose we have vectors a and b:\n\na <- c(1, 3, 5, 7)\nb <- c(1, 2, 4, 8)\n\nThen we multiply a by 5…\n\na * 5\n\n… and see that each component of a is multiplied by 5. In other words, the shorter vector (here 5) is recycled. Now multiply a with b…\n\na * b\n\n…and we see that the components in one vector matches those in the other one-for-one. Similarly for subtraction, addition and division, we get new vectors via componentwise operations. Try this here now a few times with your own vectors.\nBut what if one vector is somewhat shorter than the other? The recycling rule comes into play. If two vectors are of unequal length, the shorter one will be recycled in order to match the longer vector. For example, the following vectors u and v have different lengths, and their sum is computed by recycling values of the shorter vector u:\n\nv <- rep(2, len = 13)\nu <- rep(c(1, 20), len = 5)\nv + u"
  },
  {
    "objectID": "summary.html#dataframe-creation",
    "href": "summary.html#dataframe-creation",
    "title": "Summary",
    "section": "Dataframe creation",
    "text": "Dataframe creation\nThe most rudimentary way to create a dataframe is to create several vectors and then assemble them into a dataframe using cbind() — this is a function that combines by column. For instance:\n\n# create three vectors of different types\nvec1 <- rep(c(\"A\", \"B\", \"C\"), each = 5) # a character vector (a facctor)\nvec2 <- seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                 length.out = length(vec1)) # date vector\nvec3 <- rnorm(n = length(vec1), mean = 0, sd = 0.35) # numeric vector\n# now assemble dataframe\ndf1 <- cbind(vec1, vec2, vec3)\nhead(df1)\n\nAnother way to achieve the same thing is to use the data.frame() function that will allow you to achieve all of the above steps at once. Here is the example:\n\ndf2 <- data.frame(vec1 = rep(c(\"A\", \"B\", \"C\"), each = 5),\n                  vec2 = seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                                  length.out = length(vec1)),\n                  vec3 = rnorm(n = length(vec1), mean = 2, sd = 0.75))\nhead(df2, 2)\n\nWhat about the names of the dataframe that you just created? Are you happy that they are descriptive enough? If you aren’t, don’t fear. There are several different ways in which we can change it. We can assign the existing separate vectors vec1, vec2 and vec3 to more user-friendly names using the data.frame() function, like this:\n\ndf1 <- data.frame(level = vec1,\n                  sample.date = vec2,\n                  measurement = vec3)\nhead(df1, 2)\n\nAnother way is to change the name after you have created the dataframe using the colnames() assignment function, as in:\n\ncolnames(df2) <- c(\"level\", \"sample.date\", \"measurement\")\nhead(df2, 2)\nnames(df2)\n\nDataframes are very versatile and we can do many operations on them. A common requirement is to add a column to a dataframe that contains the outcome of some calculation. We could create a new column in the dataframe ‘on the fly’, as in:\n\ndf2.1 <- df1 # copy the dataframe\ndf2.1$meas.anom <- df1$measurement - mean(df1$measurement)\ndf2.1$meas.diff <- df2.1$measurement - df2.1$meas.anom\nhead(df2.1, 2)\n\nWe can also combine dataframes in different ways. Perhaps you have two (or more) dataframe that conform to the same layout, i.e. they have the same number of columns (although the length of the dataframes may differ), they have the same data type in those columns and the names of those columns are the same. Also, the order of the columns must be identical in all the dataframes. Two separate dataframe with the same structure may, for example, result from two identical experiments that were repeated at different times. We can then stack one on top (e.g. combine our experiments) of the other using the row bind function rbind(), as in:\n\nnrow(df1) # check the number of rows first\nnrow(df2)\ndf3 <- rbind(df1, df2)\nnrow(df3) # number of rows in the combined dataframe\nhead(df3, 2)\n\nBut now how do we know how the portions of the stacked dataframe relate to the experiments that resulted in the data in the first place? There is no label to distinguish one experiment from the other. We can fix this by adding a new column to the stacked dataframe that contains the coding for the two experiments. We can achieve it like this:\n\ndf3$exp.no <- rep(c(\"exp1\", \"exp2\"), each = nrow(df1))\nhead(df3, 2)\ntail(df3, 2)\n\nWe can combine dataframes in another way — that is, bind columns side-by-side using the function cbind(). We used it before to place vectors of the same length next to each other to create a dataframe. This function is similar to rbind(), but where rbind() fusses over the names of the columns, cbind() does not. What does concern cbind(), however, is that the number of rows in the two (or more) dataframes that will be ‘glued’ side-by-side is the same. Try it yourself with your own dataframes."
  },
  {
    "objectID": "summary.html#dataframe-indices",
    "href": "summary.html#dataframe-indices",
    "title": "Summary",
    "section": "Dataframe indices",
    "text": "Dataframe indices\nRemember that weird $ symbol we saw a little while ago? That symbol tells R that you want to see a column (vector) within a dataframe. For example, if we wanted to perform an operation on only one column in intro_data in order to ascertain the mean depth (m) of sampling:\n\nround(mean(intro_data$depth),2)\n\nIf we want to subset only specific values in a dataframe, as we have seen how to do with vectors, we need to consider that we are now working with two dimensions and not one. We still use [] but now we must do a little extra. If we want to see how long the time series for Sodwana is we could do this in several ways, here are the three most common in an improving order:\n\n# Subset a dataframe using [,]\nintro_data[12,9]\n\n# Subset only one column using []\nintro_data$length[12]\n\n# Subset from one column using logic for another column\nintro_data$length[intro_data$site == \"Sodwana\"]\n\nThe important thing to remember here is that when one needs to use a comma when subsetting, the row number is always on the left, and the column number is always on the right. Rows then columns! Tattoo that onto your brain. Or fore-arm if you are the adventurous type. We will go into the subsetting and analysis of dataframes in much more detail in the following session.\nOne must keep in mind that data in R can become substantially more complex than what we have covered, and the software also distinguishes several other kinds of data ‘containers’: in addition to vectors and dataframes, we also have lists, matrices, time series and arrays. The more complex ones, such as arrays, may have more dimensions than the two (rows along dimension 1, columns along dimension 2) that most people are familiar with. We will not delve into these here as they are bit more advanced than the goals of this course."
  },
  {
    "objectID": "R_RStudio.html",
    "href": "R_RStudio.html",
    "title": "Setting up R and Rstudio",
    "section": "",
    "text": "Before we can begin an R workshop, we need to make sure everyone has access to a computer with R and RStudio installed. The process for how to do this is detailed below, followed by a brief introduction to why we use these two pieces of software and what the difference is between them.\nNB: If you are not using your own computer please make the instructor aware of this as it is assumed that all participants will be using their personal (or university/institution etc.) laptops."
  },
  {
    "objectID": "R_RStudio.html#installing-r",
    "href": "R_RStudio.html#installing-r",
    "title": "Setting up R and Rstudio",
    "section": "Installing R",
    "text": "Installing R\nInstalling R on your machine is a straightforward process. Follow these steps:\n\nGo to the CRAN (Comprehensive R Archive Network) website. If you type ‘r’ into Google it is the first entry\nChoose to download R for Windows, Mac, or Linux\nFor Windows users selecting ‘base’ will link you to the download file, follow the prompts to install\nFor Mac users, choose the version relevant to your Operating System, follow the prompts after downloading\nIf you are a Linux user, you know what to do!"
  },
  {
    "objectID": "R_RStudio.html#installing-rstudio",
    "href": "R_RStudio.html#installing-rstudio",
    "title": "Setting up R and Rstudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nAlthough R can run in its own console or in a Terminal (Mac and Linux; the Windows command line is a bit limiting), we will use RStudio in this Workshop. RStudio is a free front-end to R for Windows, Mac, or Linux (i.e., R is working in the background). It makes working with R easier, more organised and productive, especially for new users. There are other front-ends, but RStudio is the most popular. To install:\n\nGo to the posit website\nClick the ‘Download RStudio’ button in the top right of the page\nScroll down to click the ‘Download’ button under RStudio Desktop Free\nFor Windows users click the ‘Download RStudio Desktop for Windows’ button under Step 2 on the page\nFor all other Operating Systems, scroll down further and select the corresponding file\nAfter downloading, follow the prompts to install RStudio"
  },
  {
    "objectID": "R_RStudio.html#why-r",
    "href": "R_RStudio.html#why-r",
    "title": "Setting up R and Rstudio",
    "section": "Why R?",
    "text": "Why R?\nAs scientists, we are increasingly driven to analyse and manipulate larger and larger datasets. As these datasets grow in size our analyses are becoming more sophisticated. There are many statistical packages on the market that one can use, but R is one of the global standards. There are several reasons for this:\n\nThe positives\n\nIt is free, which is nice if you despise commercial software (e.g. Microsoft Office)\nIt is powerful, flexible and robust; it is developed and used by leading academic statisticians\nIt contains advanced statistical routines not yet available in other software\nThe cutting-edge statistical routines open up scientific possibilities in creative new ways\nIt does not depend on a ‘point and click’ interface, such as SPSS, and requires one to write scripts\nIt has state-of-the-art graphics\nUsers continually extend the functionality by updating existing packages and adding new ones; in fact, this entire website was written in Quarto (RStudio) and the files supporting this Workshop material can be edited on any computer using a variety of operating systems such as Mac OS X, Linux, and Microsoft Windows\n\nIt is truly amazing that such a powerful and comprehensive software is freely available and we are indebted to the developers of R for going down this path.\n\n\nThe negatives\nAlthough there are many positives of using R, there are some negatives:\n\nIt can have a steep learning curve for those who are not into statistics or data manipulation, and it does require frequent use to remain familiar with it and to develop advanced skills\nError trapping can be confusing and frustrating\nRudimentary debugging, although there are some packages available to enhance the process\nHandles large datasets easily (eg. 100 MB), but can have some trouble with massive datasets (e.g. 100 GB)\nSome simple tasks can be tricky to do in R\nThere are multiple ways of doing the same thing\n\n\n\nThe challenges\nThe big difference between R and many other statistical packages that you might have used is that it is not, and never will be, a menu-driven ‘point and click’ software. R requires you to write your own code to tell it exactly what you want to do. This means that there is a learning curve, but these are outweighed by numerous advantages:\n\nTo write new programs, you can modify your existing ones or those of others, saving you considerable time\nYou have a record of your statistical analyses and thus can re-run your previous analyses exactly at any time in the future, even if you can’t remember what you did — this is central to reproducible research\nThe recorded code can include the liberal use of internal documentation, which is often overlooked by practicing scientists\nIt is more flexible at manipulating data and graphics than menu-driven software\nYou will develop and improve your programming, which is a valuable general skill\nYou will improve your statistical knowledge\nYou can automate large problems\nYou can provide and share code that underpins published analyses; more and more journals are requesting the code for analyses in papers, to increase transparency and reproduceability\nIntegration with tools like Git (e.g. GitHub and Bitbucket) enable online collaboration in large statistical research programmes and they allow one to rely on version control systems\nProgramming is simply heaps more fun than point-and-click!"
  },
  {
    "objectID": "R_RStudio.html#why-rstudio",
    "href": "R_RStudio.html#why-rstudio",
    "title": "Setting up R and Rstudio",
    "section": "Why RStudio?",
    "text": "Why RStudio?\nOne could, and some still do, use ‘Base R’. This is the name used to describe the R software as it is shipped from the core R team. It is run generally via a Terminal and provides very rudimentary access to help files and visuals. The Base R software hasn’t developed past this point because it is not in its mandate to do so. It is first and foremost a computer programming language. And while it is still constantly being updated and improved, it will likely never have much in the way of a user interface (UI). That is where RStudio enters the picture. It is an integrated development environment (IDE), which is more advanced than a simple UI in that it adds dozens of additional features to R that the base software itself is not capable of performing. We will not be covering most of these extended features in this workshop, but if you are interested in learning more about them feel free to ask the instructor. Though be sure that you are close to an exit door so you can escape if they don’t stop talking about all of the benefits of RStudio.\n\nGeneral settings\nBefore we start using RStudio let’s set it up properly. In the menu at the top of the RStudio software, go to: Tools (Preferences on Mac) > Global Options…. From here we have a very wide range of options for the functionality of RStudio. At the moment we will leave the general settings to their default.\n\n\nCustomising appearance\nRStudio is highly customisable. Under the Appearance tab in the Global Options you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g. ‘Chaos’) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code. Take a moment now and chose a different them before accepting and closing the window.\n\n\nThe RStudio Project\nA very nifty way of managing workflow in RStudio is through the built-in functionality of the RStudio Project. We do not need to install any packages or change any settings to use these. Creating a new project is a very simple task, as well. For this course we will be using the R_Workshop.Rproj file you will download tomorrow with the course material so that we are all running identical projects. This will prevent a lot of issues by ensuring we are doing things by the same standard. Better yet, an RStudio Project integrates seamlessly into version control software (e.g. GitHub) and allows for instant, world class collaboration on any research project. We will cover the concepts and benefits of an RStudio Project more as we move through the course.\n\n\nInstalling packages\nThe most common functions used in R are contained within the base package; this makes R useful ‘out of the box.’ However, there is extensive additional functionality that is being expanded all the time through the use of packages. Packages are simply collections of code called functions that automate complex mathematical or statistical tasks. One of the most useful features of R is that users are continuously developing new packages and making them available for free. You can find a comprehensive list of available packages on the CRAN website. There are currently (2022-11-08) 18824 packages available for R!\nIf the thought of searching for and finding R packages is daunting, a good place to start is the R Task View page. This page curates collections of packages for general tasks you might encounter, such as Experimental Design, Meta-Analysis, or Multivariate Statistics. Go and have a look for yourself, you might be surprised to find a good explanation of what you need.\nIn the menu bar click Tools > Install Packages type in the package name tidyverse in the ‘Packages’ text box (note that it is case sensitive) and select the Install button. The Console will run the code needed to install the package, and then provide some commentary on the installation of the package and any of its dependencies (i.e., other R packages needed to run the required package).\nThe installation process makes sure that the functions within the packages contained within the tidyverse are now available on your computer, but to avoid potential conflicts in the names of functions, it will not load these automatically. To make R ‘know’ about these functions in a particular session, you need either to load the package via ticking the checkbox for that package in the Packages tab, or execute:\n\nlibrary(tidyverse)\n\nSince we will develop the habit of doing all of our analyses from R scripts, it is best practice to simply list all of the libraries to be loaded right at the start of your script. Comments may be used to remind your future-self (to quote Hadley Wickham) what those packages are for.\n\nQuestion Why is it best practice to explicitly include packages you use in your R program at the start of your script?\n\n\n\nThe panes of RStudio\nRStudio has four main panes, each in a quadrant of your screen: Source Editor, Console, Environment (and History, Connections), and Plots (and Files, Packages, Help, Viewer, Presentations). The layout of these four panes can be adjusted under the Tools > Global Options…> Pane Layout menu. For now we will keep the factory default layout, but note that there might be subtle differences between RStudio installations on different operating systems. We will discuss each of the panes in turn below.\n\nSource Editor\nGenerally we will want to write chunks of code longer than a few lines. The Source Editor can help you open, edit and execute the scripts that allow us to do this. To ensure that RStudio is working as expected, follow these three steps:\n\nClick the ‘New File’ icon in the top left, just under the menu bar, then select ‘R Script’. This will open a new blank tab in the Source Editor. Save this file to your desktop and close RStudio.\nNow make RStudio the default application to open .R files (right click on the file and set RStudio to open it as the default if it isn’t already).\nDouble click on the file – this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers and there is also highlighting to help you count brackets.\n\n\nConsole\nThis is where you can type code that executes immediately. This is also known as the command line. Entering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line):\n\n6 * 3\n\n[1] 18\n\n5 + 4\n\n[1] 9\n\n2 ^ 3\n\n[1] 8\n\n\n\nNote that spaces are optional around simple calculations.\n\nWe can also use the assignment operator <- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it’s bad practice to use it… and we’ll talk about this as we go):\n\na <- 2\nb <- 7\na + b\n\n[1] 9\n\n\nTo type the assignment operator (<-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd<-2\nd < -2\n\n[1] FALSE\n\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we hadn’t used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\nWe can create a vector in R by using the combine c() function:\n\napples <- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We’ll talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\n[1] 5.3 3.8 4.5\n\n\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\n[1] 4.533333\n\nsd(apples)\n\n[1] 0.7505553\n\n\n\nVariable names\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\nOr try this:\n\nround(sd(apples), 2)\n\n[1] 0.75\n\n\n\nQuestion\nWhat did we do above? What can you conclude from those functions?\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief inline help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser or Google. On a blank line in the Console, press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)\n\n\n\nEnvironment and History panes\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values, and functions) you have in your environment (i.e. workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line or highlight a block of lines and then double-click those, you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you’ve loaded into the Environment:\n\nls()\n\n[1] \"a\"        \"apples\"   \"b\"        \"d\"        \"pkgs_lst\" \"url\"     \n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?\n\n\nFiles, Plots, Packages, Help, Viewer, and Presentation panes\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed. The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console. The Viewer tab is where more advanced/interactive visuals are generated, and if one is authoring presentations in RStudio, they will be visualised in the Presentation pane.\nMethods of getting help from the Console include…\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in R Workflow - I."
  },
  {
    "objectID": "R_RStudio.html#resources",
    "href": "R_RStudio.html#resources",
    "title": "Setting up R and Rstudio",
    "section": "Resources",
    "text": "Resources\nBelow you can find the source code to some books and other links to websites about R:\n\nggplot2. Elegant Graphics for Data Analysis — the gold-standard in R graphics\nR for Data Science — data analysis using tidy principles\nR Markdown — reproducible reports in R\nbookdown: Authoring Books and Technical Documents with R — writing books in R\nShiny — interactive web apps driven by R"
  },
  {
    "objectID": "R_RStudio.html#exercise",
    "href": "R_RStudio.html#exercise",
    "title": "Setting up R and Rstudio",
    "section": "Exercise",
    "text": "Exercise\n\nIt which shall not be named\n\nSlides\n\n\nSource"
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "Tidy Data",
    "section": "",
    "text": "The Tidyverse is a collection of R packages that adhere to the tidy data principles of data analysis and graphing. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the years others have added more to the collective, which has significantly expanded our data analytical capabilities through improved ease of use and efficiency. The Tidyverse packages can be loaded collectively by calling the tidyverse package, as we have seen throughout this workshop. Over the following three slide decks and exercises we will se a range of basic to intermediate methods of wrangling data. From tidy examples to how to deal with messier data."
  },
  {
    "objectID": "tidy.html#slides-and-application-exercises",
    "href": "tidy.html#slides-and-application-exercises",
    "title": "Tidy Data",
    "section": "Slides and application exercises",
    "text": "Slides and application exercises\n\nTidy 1: Wrangling\n\nSlides\n\n\nSource\n\n\n\nWoooo there!\n\nSource\n\n\n\nTidy 2: Taming\n\nSlides\n\n\nSource\n\n\n\nSettle down now\n\nSource\n\n\n\nTidy 3: Domesticating\n\nSlides\n\n\nSource\n\n\n\nWhose a good data!?\n\nSource"
  },
  {
    "objectID": "tidy.html#diy-tidy-data",
    "href": "tidy.html#diy-tidy-data",
    "title": "Tidy Data",
    "section": "DIY tidy data",
    "text": "DIY tidy data\nNow that we have learned the code for basic to intermediate tidy analyses, let’s apply this to our own data. Find a dataset somewhere on your computer, load, wrangle, and analyse it in a way that is interesting to you. A particularly interesting idea is to use the code we’ve learned thus far to recreate an analysis + visualisation from an old assignment, project, or publication that we performed manually in Excel (or something similar)."
  },
  {
    "objectID": "GitHub.html",
    "href": "GitHub.html",
    "title": "GitHub with RStudio",
    "section": "",
    "text": "It may seem like a detour to be setting up a GitHub account on our way to an R workshop, but this online tool is centrally important to the best practices for working in RStudio, as we shall see below. If you already have a GitHub account, wonderful! Much of the following information is probably already known to you. So rather spend this time helping the people next to you through the process. Much of the content in this module is taken from the wonderful online book Happy Git and GitHub for the useR, which provides a much more in-depth view of what Git is, the many ways we can use it, and many tips and tricks for any errors encountered below. For practical purposes, in this module we are going to focus directly on what we need to know to connect RStudio to GitHub in order to manage RStudio Projects.\nAt the outset of this module we must acknowledge that there are many other Git based online (and desktop) resources that one may use in an R/RStudio workflow. We choose to focus on GitHub in this workshop because it is the best integrated, supported, and documented option. Meaning that not only is it the easiest to use ‘out of the box’, if we encounter any issues after this workshop it will be easiest to search for solutions for GitHub. Note however that GitHub is owned by Microsoft, which is why many people have switched over to GitLab. If anyone is interested in the finer points of this discussion, please let the instructor know."
  },
  {
    "objectID": "GitHub.html#create-a-github-account",
    "href": "GitHub.html#create-a-github-account",
    "title": "GitHub with RStudio",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nAs with any online resource in the digital age, we must first setup an account. To do so, go to the GitHub home page, click ‘Sign up’ in the top right, and follow the prompts. You will need an e-mail address to complete this process, but there is otherwise no cost BONUS: if you have an academic e-mail address you should be able to register for an academic account, which provides some of the benefits of a paid account, but for free! If you experience any issues in this process try consulting the GitHub help page."
  },
  {
    "objectID": "GitHub.html#install-git",
    "href": "GitHub.html#install-git",
    "title": "GitHub with RStudio",
    "section": "Install Git",
    "text": "Install Git\nThe Git software comes standard on MacOS and all Linux distributions. If you have one of these Operating Systems, run the following code in a Bash shell (i.e. Terminal) to check that you have it:\nwhich git\nAnd if you do, check your version:\ngit --version\nClose RStudio before proceeding with the installation of Git for a specific Operating System.\n\nWindows\nThere is a one-stop-shop for all of your Git needs in the form of Git for Windows. Navigate to this page, download the file, and follow the prompts to install. Generally the default options are acceptable, with two important considerations:\n\nFor the ‘Adjusting your PATH environment’ prompt, ensure that you choose ‘Git from the command line and also from 3rd-party software’\nThe ideal location to install Git is C:/Program Files, doing otherwise may cause problems later on\n\nIf you already have Git installed for Windows, run the following line in your Terminal to check that it is up-to-date:\ngit update-git-for-windows\n\n\nMacOS\nOpen a Terminal and run:\nxcode-select --install\n\n\nLinux\nUbuntu or Debian Linux:\nsudo apt-get install git\nFedora or RedHat Linux:\nsudo yum install git\n\n\nSet user info\nWith Git installed, we need to tell it our username and e-mail. This isn’t linked to anything else on our computer, but it will appear during our workflow between RStudio and Github. Primarily this information is useful when we are working on a project with other people so they can see who made what changes when. Preferably one should use the same e-mail account here as that for used for the GitHub account.\nIn a Terminal:\ngit config --global user.name 'Jane Doe'\ngit config --global user.email 'jane@example.com'\ngit config --global --list"
  },
  {
    "objectID": "GitHub.html#ssh-key",
    "href": "GitHub.html#ssh-key",
    "title": "GitHub with RStudio",
    "section": "SSH key",
    "text": "SSH key\nWhile it is possible to connect RStudio to GitHub using the username and password we created above for GitHub (i.e. HTTPS connection), this quickly becomes tedious because we must provide this info every time we upload something. Rather it is better to spend a few minutes now setting up an SSH key so that we can focus more on our work, and not remembering usernames and passwords. There are best practices on the management of SSH keys (i.e. changing them once a year), but we won’t get into that here.\n\nCreate a key\nOpen RStudio and go to: Tools > Global Options…> Git/SVN > Create RSA Key…. Note that if there is a file pathway in the text box above the ‘Create RSA Key…’ button this means you already have an SSH key. If not, click ‘Create’ and RStudio will generate one for you. It will prompt you for a passphrase. This isn’t necessary for now and can be skipped. If you want to create a new SSH key with a passphrase later circle back to this step. Adding a passphrase will require additional steps that are not listed below and can be found here.\n\n\nAdd key to ssh-agent\n\nMacOS\nCheck that ssh-agent is enabled (the pid may vary):\n~ % eval \"$(ssh-agent -s)\"\nOn MacOS Sierra 10.12.2 and higher create a ~/.ssh/config file with this text inside:\nHost *\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_ed25519\n\n\n\nWindows\nIn the Git Bash Shell:\n$ eval $(ssh-agent -s)\nAdd your key (change the name to match whatever you chose above):\n$ ssh-add ~/.ssh/id_ed25519\n\n\nLinux\nCheck that ssh-agent is running:\n$ eval \"$(ssh-agent -s)\"\nAdd your key (change name as necessary):\nssh-add ~/.ssh/id_ed25519"
  },
  {
    "objectID": "GitHub.html#ssh-to-github",
    "href": "GitHub.html#ssh-to-github",
    "title": "GitHub with RStudio",
    "section": "SSH to GitHub",
    "text": "SSH to GitHub\nIn RStudio: Tools > Global Options…> Git/SVN.. If your key is connected, there should be a ‘View public key’ option to click on. Do so and accept the offer to copy to clipboard. Or do so manually if not prompted.\nIn GitHub: Settings > SSH and GPG keys. Click the green ‘New SSH key’ button in the top right. Paste the SSH key you copied from RStudio in the ‘Key’ box and give it a name that makes sense to you. Finish by clicking ‘Add SSH key’."
  },
  {
    "objectID": "GitHub.html#github-and-rstudio",
    "href": "GitHub.html#github-and-rstudio",
    "title": "GitHub with RStudio",
    "section": "GitHub and RStudio",
    "text": "GitHub and RStudio\nNow that we have Git sorted on our computers and we have created an SSH key for GitHub we may connect our RStudio Projects to GitHub. To do so we must first create a new repository. On your GitHub account click the green ‘New’ button.\nStart with the following info for testing purposes:\n\nRepository name: winter_school_yourname (or whatever you like)\nDescription: Repository for code and notes taken during FACE-IT 2022 winter R workshop\nPublic\nInitialize this repository with: (X) Add a README file\n\nThen click the green Create repository button.\nTo connect this repo to our RStudio we must first copy the link. Do so by clicking on the green ‘Code’ button in the top right. Select the ‘SSH’ option and cop the text below. It should look something like: git@github.com:yourusername/winter_school_yourname.git\nIn RStudio: File > New Project > Version Control > Git and paste the link in ‘Repository URL’. Before you click ’ Create Project’ note where RStudio intends to save the files and change this if desired.\nYou should now see a window open that shows that RStudio is downloading your files to your computer. Let’s open the README.md file in RStudio, add a bit of text, and save the changes. Now that we have made changes we can upload them back to GitHub. This is done via the Git tab in the Environment pane, which is in the top right by default.\n\nSpecific terms\nThe process of uploading the changes to our code to GitHub is not complex, but the technical terms used are very generic and easy to confuse. Therefore we have them set out below in bullet points for ease of reference over the following week of the workshop:\n\nCommit: This is the term used to describe the process of saving any changes you have made to your code (and files etc.) locally on your computer\n\nTo access this click the ‘Commit’ button within the Git tab\nThis opens a new window showing which files where changed, and which lines were added (green) or deleted (red)\nClick the check box next to each file you want to commit and then write a short message explaining what was done\nClicking the ‘Commit’ button will save these changes and the attached message locally on your machine via the Git software\n\nPush: This is the term used to describe the process of uploading data from your local machine to your GitHub account\n\nNote that you cannot push anything until you have commit it locally\n\nPull: This is how we download new code, files, etc. from our GitHub repository to our local machine\n\nThis is generally used when we are collaborating with other people\nIf you are only working by yourself, on just one computer, you won’t use this often"
  },
  {
    "objectID": "GitHub.html#exercise-1",
    "href": "GitHub.html#exercise-1",
    "title": "GitHub with RStudio",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nPush your repo from RStudio to GitHub\nUsing the definitions for the key-terms above successfully upload your changes to Github. Once you think you have succeeded, go to the repo page on your GitHub account and check that the changes have appeared."
  },
  {
    "objectID": "GitHub.html#more-options",
    "href": "GitHub.html#more-options",
    "title": "GitHub with RStudio",
    "section": "More options",
    "text": "More options\nThere are many things that can be done via Git, GitHub, and/or RStudio. We won’t go into all of them here, but we will look at one example. Go to the repository for this workshop and copy the ‘Code’ link. Start a new RStudio Project following the same steps as above but use the new link.\nAfter following the prompts it should download all of the source code for this site, and the extra course content, to your local computer. This is just one example of how we can share code and documents with a wide range of collaborators."
  },
  {
    "objectID": "GitHub.html#exercise-2",
    "href": "GitHub.html#exercise-2",
    "title": "GitHub with RStudio",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nCollaboration via GitHub\nPair up with a buddy and practice pulling each others repositories, making changes, and pushing them back. Once you are comfortable with this, find a new buddy and repeat the process."
  },
  {
    "objectID": "brief.html",
    "href": "brief.html",
    "title": "Policy Brief",
    "section": "",
    "text": "To be determined."
  }
]